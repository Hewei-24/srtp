"""
å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - ä¸»æœåŠ¡å™¨
=====================================

æœ¬æ¨¡å—æä¾›åŸºäº Flask çš„ Web æœåŠ¡ï¼Œé›†æˆä»¥ä¸‹åŠŸèƒ½ï¼š
1. DeepSeek API å¿ƒç†å’¨è¯¢æœåŠ¡
2. DeepFace é¢éƒ¨è¡¨æƒ…è¯†åˆ«
3. å¯¹è¯å†å²ç®¡ç†
4. RESTful API æ¥å£
5. æ•°å­—äººå½¢è±¡é€‰æ‹©åŠŸèƒ½
6. è¯­éŸ³è¾“å…¥è¯†åˆ«åŠŸèƒ½

ä½œè€…: SRTP é¡¹ç›®ç»„
ç‰ˆæœ¬: 2.2
"""

import os
import base64
import logging
import datetime
import subprocess
import uuid
import glob
import io
import wave
import tempfile
from typing import Dict, Any, Optional

import cv2
import numpy as np
import requests
from flask import Flask, request, jsonify, send_from_directory, redirect, url_for
from flask_cors import CORS

# è¯­éŸ³è¯†åˆ«ç›¸å…³å¯¼å…¥
try:
    import speech_recognition as sr
    import pydub
    from pydub import AudioSegment
    SPEECH_RECOGNITION_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("è¯­éŸ³è¯†åˆ«åº“åŠ è½½æˆåŠŸ")
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…ï¼Œè¯­éŸ³è¾“å…¥åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install SpeechRecognition pydub")

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== DeepFace å¯¼å…¥ ====================
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    logger.info("DeepFace åº“åŠ è½½æˆåŠŸ")
except ImportError:
    DEEPFACE_AVAILABLE = False
    logger.warning("DeepFace åº“æœªå®‰è£…ï¼Œè¡¨æƒ…è¯†åˆ«åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install deepface")

# ==================== Flask åº”ç”¨åˆå§‹åŒ– ====================
app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# ==================== é…ç½®å¸¸é‡ ====================
# DeepSeek API é…ç½®ï¼ˆå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥ï¼‰
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-215440b00f1d426fb21a2f11eef6cf02")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# TTS API é…ç½® (SiliconFlow)
TTS_API_URL = "https://api.siliconflow.cn/v1/audio/speech"
TTS_API_TOKEN = "sk-lvtuhfndddcmdyvnjtbzjuobfoewylsnqaqwfsnuznpilhkp"

# SadTalker é…ç½®
SADTALKER_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "SadTalker")
SADTALKER_IMAGE = os.path.join(SADTALKER_DIR, "my_photo.png")  # æ•°å­—äººå›¾ç‰‡ï¼ˆé»˜è®¤ï¼‰
SADTALKER_OUTPUT_DIR = os.path.join(SADTALKER_DIR, "results")
AUDIO_OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "audio_output")
AVATARS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "avatars")
SPEECH_INPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "speech_input")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)
os.makedirs(SADTALKER_OUTPUT_DIR, exist_ok=True)
os.makedirs(AVATARS_DIR, exist_ok=True)
os.makedirs(SPEECH_INPUT_DIR, exist_ok=True)

# æ£€æŸ¥ avatars ç›®å½•ä¸‹æ˜¯å¦æœ‰é»˜è®¤å›¾ç‰‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
default_avatar_path = os.path.join(AVATARS_DIR, "avatar1.png")
if not os.path.exists(default_avatar_path):
    # å°† SadTalker çš„é»˜è®¤å›¾ç‰‡å¤åˆ¶åˆ° avatars ç›®å½•ä½œä¸º avatar1
    if os.path.exists(SADTALKER_IMAGE):
        import shutil
        shutil.copy2(SADTALKER_IMAGE, default_avatar_path)
        logger.info(f"å·²å°†é»˜è®¤æ•°å­—äººå›¾ç‰‡å¤åˆ¶åˆ°: {default_avatar_path}")
    else:
        # åˆ›å»ºä¸‰ä¸ªç¤ºä¾‹å›¾ç‰‡è·¯å¾„
        for i in range(1, 4):
            avatar_path = os.path.join(AVATARS_DIR, f"avatar{i}.png")
            if not os.path.exists(avatar_path):
                logger.warning(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {avatar_path}ï¼Œè¯·æ”¾ç½®ç›¸åº”å›¾ç‰‡æ–‡ä»¶")

# æƒ…ç»ªæ˜ å°„è¡¨
EMOTION_MAP = {
    'angry':    {'icon': 'ğŸ˜ ', 'name': 'ç”Ÿæ°”', 'context': 'æœ‰äº›ç”Ÿæ°”'},
    'disgust':  {'icon': 'ğŸ¤¢', 'name': 'åŒæ¶', 'context': 'æœ‰äº›åæ„Ÿ'},
    'fear':     {'icon': 'ğŸ˜¨', 'name': 'ææƒ§', 'context': 'æ„Ÿåˆ°ç´§å¼ '},
    'happy':    {'icon': 'ğŸ˜Š', 'name': 'å¼€å¿ƒ', 'context': 'çœ‹èµ·æ¥å¿ƒæƒ…ä¸é”™'},
    'sad':      {'icon': 'ğŸ˜¢', 'name': 'æ‚²ä¼¤', 'context': 'æƒ…ç»ªæœ‰äº›ä½è½'},
    'surprise': {'icon': 'ğŸ˜²', 'name': 'æƒŠè®¶', 'context': 'æœ‰äº›æƒŠè®¶'},
    'neutral':  {'icon': 'ğŸ˜', 'name': 'å¹³é™', 'context': 'æƒ…ç»ªå¹³ç¨³'}
}

# é»˜è®¤æƒ…ç»ªåˆ†æ•°ï¼ˆå½“æ£€æµ‹å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
DEFAULT_EMOTION_SCORES = {
    'angry': 0.0, 'disgust': 0.0, 'fear': 0.0,
    'happy': 0.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 100.0
}

# å¯¹è¯å†å²ï¼ˆå…¨å±€å˜é‡ï¼‰
conversation_history: list = []


# ==================== è¯­éŸ³è¯†åˆ«æ¨¡å— ====================
def recognize_speech_from_audio(audio_data: bytes, audio_format: str = "webm") -> Dict[str, Any]:
    """
    ä»éŸ³é¢‘æ•°æ®ä¸­è¯†åˆ«è¯­éŸ³
    
    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        audio_format: éŸ³é¢‘æ ¼å¼ï¼ˆwebm, wav, mp3ç­‰ï¼‰
        
    Returns:
        åŒ…å«è¯†åˆ«ç»“æœçš„å­—å…¸
    """
    if not SPEECH_RECOGNITION_AVAILABLE:
        return {
            "success": False,
            "error": "è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…",
            "text": ""
        }
    
    try:
        recognizer = sr.Recognizer()
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
        with tempfile.NamedTemporaryFile(suffix=f".{audio_format}", delete=False) as tmp_file:
            tmp_file.write(audio_data)
            tmp_file_path = tmp_file.name
        
        try:
            # ä½¿ç”¨ pydub åŠ è½½éŸ³é¢‘æ–‡ä»¶
            if audio_format == "webm":
                audio = AudioSegment.from_file(tmp_file_path, format="webm")
            elif audio_format == "mp3":
                audio = AudioSegment.from_mp3(tmp_file_path)
            elif audio_format == "wav":
                audio = AudioSegment.from_wav(tmp_file_path)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
                audio = AudioSegment.from_file(tmp_file_path)
            
            # è½¬æ¢ä¸º wav æ ¼å¼ï¼ˆSpeechRecognition éœ€è¦ï¼‰
            wav_data = io.BytesIO()
            audio.export(wav_data, format="wav")
            wav_data.seek(0)
            
            # ä½¿ç”¨ SpeechRecognition è¯†åˆ«
            with sr.AudioFile(wav_data) as source:
                # è°ƒæ•´ç¯å¢ƒå™ªå£°
                recognizer.adjust_for_ambient_noise(source, duration=0.5)
                audio_data = recognizer.record(source)
                
                # è¯†åˆ«è¯­éŸ³
                text = recognizer.recognize_google(audio_data, language="zh-CN")
                
                logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {text}")
                
                return {
                    "success": True,
                    "text": text,
                    "confidence": 0.9  # æš‚æ—¶ä½¿ç”¨å›ºå®šå€¼
                }
                
        except sr.UnknownValueError:
            return {
                "success": False,
                "error": "æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹",
                "text": ""
            }
        except sr.RequestError as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}",
                "text": ""
            }
        except Exception as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«å¤„ç†é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"å¤„ç†é”™è¯¯: {str(e)}",
                "text": ""
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_file_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "text": ""
        }


def save_audio_file(audio_data: bytes, filename: str) -> str:
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°
    
    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        filename: æ–‡ä»¶å
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    file_path = os.path.join(SPEECH_INPUT_DIR, filename)
    with open(file_path, 'wb') as f:
        f.write(audio_data)
    return file_path


# ==================== å¿ƒç†åˆ†æä»£ç†ç±» ====================
class PsychologicalAgent:
    """
    å¿ƒç†åˆ†æä»£ç†ç±»

    è´Ÿè´£ä¸ DeepSeek API äº¤äº’ï¼Œæä¾›å¿ƒç†å’¨è¯¢æœåŠ¡ã€‚
    æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç»“åˆç”¨æˆ·æƒ…ç»ªçŠ¶æ€ç”Ÿæˆä¸ªæ€§åŒ–å›å¤ã€‚
    """

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–å¿ƒç†åˆ†æä»£ç†

        Args:
            api_key: DeepSeek API å¯†é’¥
        """
        self.api_key = api_key
        self.api_url = DEEPSEEK_API_URL

    def _build_system_prompt(self, emotion: str) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯

        Args:
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

        Returns:
            åŒ…å«æƒ…ç»ªä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯
        """
        emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
        emotion_context = emotion_info['context']

        return f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¤§å­¦å¿ƒç†å¥åº·é¡¾é—®ï¼Œä¸“é—¨å¸®åŠ©å¤§å­¦ç”Ÿè§£å†³å¿ƒç†é—®é¢˜ã€‚

é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- ç³»ç»Ÿæ£€æµ‹åˆ°ç”¨æˆ·å½“å‰çš„æƒ…ç»ªçŠ¶æ€ä¸ºï¼š{emotion} ({emotion_context})
- è¿™ä¸ªæƒ…ç»ªä¿¡æ¯æ¥è‡ªå®æ—¶é¢éƒ¨è¡¨æƒ…åˆ†æ
- è¯·ç»“åˆç”¨æˆ·æè¿°çš„æ–‡å­—å†…å®¹å’Œæ£€æµ‹åˆ°çš„æƒ…ç»ªçŠ¶æ€ï¼Œæä¾›æ›´ç²¾å‡†çš„å¿ƒç†åˆ†æ

ä½ çš„èŒè´£ï¼š
1. åˆ†æå­¦ç”Ÿçš„å¿ƒç†çŠ¶æ€å’Œæƒ…ç»ªé—®é¢˜
2. æä¾›ä¸“ä¸šã€æ¸©æš–çš„å¿ƒç†æ”¯æŒå’Œå»ºè®®
3. è¯†åˆ«å±æœºæƒ…å†µå¹¶ç»™å‡ºé€‚å½“å»ºè®®
4. ç”¨åŒç†å¿ƒå’Œç†è§£æ¥å›åº”ç”¨æˆ·

è¯·ä»¥æ¸©æš–ã€ä¸“ä¸šã€æ”¯æŒæ€§çš„è¯­æ°”å›åº”ï¼Œé¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€æä¾›å»ºè®®ã€‚"""

    def analyze(self, user_input: str, emotion: str = "neutral") -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå¿ƒç†å’¨è¯¢å›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹ï¼Œé»˜è®¤ä¸º neutral

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ successã€responseã€model_source ç­‰å­—æ®µ
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": self._build_system_prompt(emotion)}]

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š 3 è½®ï¼Œå³ 6 æ¡æ¶ˆæ¯ï¼‰
            if conversation_history:
                messages.extend(conversation_history[-6:])

            messages.append({"role": "user", "content": user_input})

            logger.info(f"è°ƒç”¨ DeepSeek API - æƒ…ç»ª: {emotion}, è¾“å…¥: {user_input[:50]}...")

            # è°ƒç”¨ API
            response = requests.post(
                self.api_url,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": messages,
                    "temperature": 0.7,  # æ§åˆ¶å›å¤çš„éšæœºæ€§
                    "max_tokens": 800,   # é™åˆ¶å›å¤é•¿åº¦
                    "stream": False
                },
                timeout=30
            )

            # å¤„ç†å“åº”
            if response.status_code == 200:
                result = response.json()
                assistant_response = result['choices'][0]['message']['content']

                # æ›´æ–°å¯¹è¯å†å²
                self._update_history(user_input, assistant_response)

                return {
                    "success": True,
                    "response": assistant_response,
                    "model_source": "deepseek_api"
                }
            else:
                logger.error(f"API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text[:200]}")
                return {
                    "success": False,
                    "error": f"API é”™è¯¯: {response.status_code}",
                    "response": "æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
                }

        except requests.exceptions.Timeout:
            logger.error("API è¯·æ±‚è¶…æ—¶")
            return {
                "success": False,
                "error": "è¯·æ±‚è¶…æ—¶",
                "response": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•ã€‚"
            }
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": "ç³»ç»Ÿæš‚æ—¶å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            }

    def _update_history(self, user_input: str, assistant_response: str) -> None:
        """
        æ›´æ–°å¯¹è¯å†å²

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            assistant_response: AI å›å¤
        """
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": assistant_response})

        # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘ 10 æ¡ï¼‰
        if len(conversation_history) > 10:
            conversation_history[:] = conversation_history[-10:]


# ==================== è¡¨æƒ…è¯†åˆ«æ¨¡å— ====================
def analyze_emotion_from_image(image_data: str) -> Dict[str, Any]:
    """
    ä» Base64 ç¼–ç çš„å›¾åƒä¸­åˆ†æé¢éƒ¨è¡¨æƒ…

    Args:
        image_data: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        åŒ…å«æƒ…ç»ªåˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ dominant_emotionã€emotion_scoresã€face_detected
    """
    # æ£€æŸ¥ DeepFace æ˜¯å¦å¯ç”¨
    if not DEEPFACE_AVAILABLE:
        logger.warning("DeepFace åº“ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤æƒ…ç»ª")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }

    logger.info("å¼€å§‹ DeepFace è¡¨æƒ…åˆ†æ...")

    try:
        # è§£ç  Base64 å›¾åƒ
        if ',' in image_data:
            image_data = image_data.split(',')[1]

        img_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img is None:
            logger.warning("å›¾åƒè§£ç å¤±è´¥")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # ä½¿ç”¨ DeepFace åˆ†æè¡¨æƒ…
        analysis = DeepFace.analyze(
            img,
            actions=['emotion'],
            detector_backend='opencv',  # ä½¿ç”¨ OpenCV æ£€æµ‹å™¨ï¼ˆé€Ÿåº¦å¿«ï¼‰
            enforce_detection=False,    # ä¸å¼ºåˆ¶æ£€æµ‹åˆ°äººè„¸
            silent=True                 # é™é»˜æ¨¡å¼
        )

        if not analysis:
            logger.warning("DeepFace è¿”å›ç©ºç»“æœ")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # æå–ç»“æœ
        dominant_emotion = analysis[0]['dominant_emotion']
        emotion_scores = analysis[0]['emotion']

        # æ£€æŸ¥æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸ï¼ˆé€šè¿‡æ£€æŸ¥ face_confidence æˆ– regionï¼‰
        face_region = analysis[0].get('region', {})
        face_confidence = analysis[0].get('face_confidence', 0)

        # è®°å½•è¯¦ç»†çš„åˆ†æç»“æœç”¨äºè°ƒè¯•
        logger.info(f"è¡¨æƒ…åˆ†æç»“æœ: dominant={dominant_emotion}, scores={emotion_scores}")
        logger.info(f"äººè„¸åŒºåŸŸ: {face_region}, ç½®ä¿¡åº¦: {face_confidence}")

        # è½¬æ¢ä¸º Python float ç±»å‹ï¼ˆé¿å… JSON åºåˆ—åŒ–é—®é¢˜ï¼‰
        converted_scores = {k: float(v) for k, v in emotion_scores.items()}

        # åˆ¤æ–­æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸
        # å¦‚æœäººè„¸åŒºåŸŸå¤ªå°æˆ–ç½®ä¿¡åº¦å¤ªä½ï¼Œå¯èƒ½æ˜¯è¯¯æ£€
        face_detected = True
        if face_region:
            w = face_region.get('w', 0)
            h = face_region.get('h', 0)
            # å¦‚æœæ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°ï¼ˆå°äº50x50åƒç´ ï¼‰ï¼Œè®¤ä¸ºæ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸
            if w < 50 or h < 50:
                logger.warning(f"æ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°: {w}x{h}")
                face_detected = False

        return {
            "dominant_emotion": dominant_emotion,
            "emotion_scores": converted_scores,
            "face_detected": face_detected
        }

    except Exception as e:
        logger.warning(f"è¡¨æƒ…åˆ†æå¤±è´¥: {str(e)[:100]}")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }


def generate_fallback_response(user_input: str, emotion: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¤‡é€‰å›å¤ï¼ˆå½“ API ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ª

    Returns:
        åŒ…å«å¤‡é€‰å›å¤çš„å­—å…¸
    """
    emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
    emotion_prefix = f"{emotion_info['context']}ï¼Œ" if emotion != 'neutral' else ""

    # å…³é”®è¯åŒ¹é…å›å¤
    keyword_responses = {
        'å‹åŠ›': f"{emotion_prefix}å¯¹äºå‹åŠ›é—®é¢˜ï¼Œå»ºè®®ï¼š<br>1. æ·±å‘¼å¸æ”¾æ¾ç»ƒä¹ <br>2. åˆç†å®‰æ’æ—¶é—´å’Œä¼˜å…ˆçº§<br>3. é€‚é‡è¿åŠ¨é‡Šæ”¾å‹åŠ›<br>4. ä¸æœ‹å‹æˆ–å®¶äººå€¾è¯‰",
        'ç„¦è™‘': f"{emotion_prefix}åº”å¯¹ç„¦è™‘çš„æ–¹æ³•ï¼š<br>1. æ­£å¿µå†¥æƒ³ç»ƒä¹ <br>2. å†™ä¸‹æ‹…å¿§äº‹é¡¹<br>3. æ¸è¿›å¼è‚Œè‚‰æ”¾æ¾<br>4. ä¿æŒè§„å¾‹ä½œæ¯",
        'å¤±çœ ': f"{emotion_prefix}æ”¹å–„ç¡çœ çš„å»ºè®®ï¼š<br>1. ç¡å‰1å°æ—¶ä¸ä½¿ç”¨ç”µå­è®¾å¤‡<br>2. åˆ›é€ èˆ’é€‚çš„ç¡çœ ç¯å¢ƒ<br>3. ä¿æŒè§„å¾‹çš„ä½œæ¯æ—¶é—´<br>4. é¿å…ç¡å‰æ‘„å…¥å’–å•¡å› ",
        'æŠ‘éƒ': f"{emotion_prefix}å¦‚æœæŒç»­æƒ…ç»ªä½è½ï¼š<br>1. å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢<br>2. ä¿æŒé€‚åº¦çš„ç¤¾äº¤æ´»åŠ¨<br>3. åšæŒé€‚é‡è¿åŠ¨<br>4. ç»™è‡ªå·±ä¸€äº›æ—¶é—´å’Œè€å¿ƒ",
        'å­¦ä¹ ': f"{emotion_prefix}å­¦ä¹ å‹åŠ›ç®¡ç†ï¼š<br>1. åˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’<br>2. ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•æé«˜æ•ˆç‡<br>3. ä¿è¯å……è¶³çš„ä¼‘æ¯æ—¶é—´<br>4. ä¸åŒå­¦äº¤æµå­¦ä¹ å¿ƒå¾—"
    }

    # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
    for keyword, response in keyword_responses.items():
        if keyword in user_input.lower():
            return {
                "success": True,
                "response": response,
                "detected_emotion": emotion,
                "model_source": "fallback_system",
                "timestamp": datetime.datetime.now().isoformat()
            }

    # é€šç”¨å›å¤
    return {
        "success": True,
        "response": f"{emotion_prefix}æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ã€‚ä½œä¸ºå¿ƒç†åŠ©æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥æ›´è¯¦ç»†åœ°æè¿°å…·ä½“æƒ…å†µå’Œæ„Ÿå—ï¼Œè¿™æ ·æˆ‘èƒ½æä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„å¸®åŠ©ã€‚",
        "detected_emotion": emotion,
        "model_source": "fallback_system",
        "timestamp": datetime.datetime.now().isoformat()
    }


def test_deepseek_api() -> Dict[str, Any]:
    """
    æµ‹è¯• DeepSeek API è¿æ¥çŠ¶æ€

    Returns:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
            },
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 5
            },
            timeout=10
        )

        if response.status_code == 200:
            return {"success": True, "message": "API è¿æ¥æ­£å¸¸"}
        elif response.status_code == 401:
            return {"success": False, "message": "API å¯†é’¥æ— æ•ˆ"}
        else:
            return {"success": False, "message": f"API è¿”å›é”™è¯¯: {response.status_code}"}

    except requests.exceptions.Timeout:
        return {"success": False, "message": "API è¿æ¥è¶…æ—¶"}
    except Exception as e:
        return {"success": False, "message": f"API è¿æ¥å¤±è´¥: {str(e)}"}


# ==================== TTS æ–‡å­—è½¬è¯­éŸ³æ¨¡å— ====================
def text_to_speech(text: str) -> Dict[str, Any]:
    """
    å°†æ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³ MP3 æ–‡ä»¶

    Args:
        text: è¦è½¬æ¢çš„æ–‡å­—å†…å®¹

    Returns:
        åŒ…å«éŸ³é¢‘æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        audio_filename = f"tts_{uuid.uuid4().hex[:8]}.mp3"
        audio_path = os.path.join(AUDIO_OUTPUT_DIR, audio_filename)

        logger.info(f"å¼€å§‹ TTS è½¬æ¢ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")

        # è°ƒç”¨ SiliconFlow TTS API
        request_data = {
            "model": "IndexTeam/IndexTTS-2",
            "voice": "IndexTeam/IndexTTS-2:claire",
            "stream": True,
            "input": text,
            "max_tokens": 1600,
            "response_format": "mp3",
            "speed": 1,
            "gain": 0
        }
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {TTS_API_TOKEN}"
        }

        response = requests.post(
            url=TTS_API_URL,
            json=request_data,
            headers=headers,
            timeout=60
        )

        if response.status_code != 200:
            logger.error(f"TTS API é”™è¯¯: {response.status_code} - {response.text[:200]}")
            return {"success": False, "error": f"TTS API é”™è¯¯: {response.status_code}"}

        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        with open(audio_path, 'wb') as f:
            f.write(response.content)

        logger.info(f"TTS è½¬æ¢æˆåŠŸï¼Œä¿å­˜åˆ°: {audio_path}")
        return {
            "success": True,
            "audio_path": audio_path,
            "audio_filename": audio_filename
        }

    except requests.exceptions.Timeout:
        logger.error("TTS API è¯·æ±‚è¶…æ—¶")
        return {"success": False, "error": "TTS è¯·æ±‚è¶…æ—¶"}
    except Exception as e:
        logger.error(f"TTS è½¬æ¢å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# ==================== SadTalker è§†é¢‘ç”Ÿæˆæ¨¡å— ====================
def generate_talking_video(audio_path: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ SadTalker ç”Ÿæˆæ•°å­—äººè¯´è¯è§†é¢‘

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶çš„ç»å¯¹è·¯å¾„

    Returns:
        åŒ…å«è§†é¢‘æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(SADTALKER_IMAGE):
            logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {SADTALKER_IMAGE}")
            return {"success": False, "error": "æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨"}

        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return {"success": False, "error": "éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}

        logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
        logger.info(f"å›¾ç‰‡: {SADTALKER_IMAGE}")
        logger.info(f"éŸ³é¢‘: {audio_path}")

        # æ£€æµ‹ SadTalker ç›®å½•ä¸‹çš„è™šæ‹Ÿç¯å¢ƒ
        sadtalker_venv_python = os.path.join(SADTALKER_DIR, ".venv", "Scripts", "python.exe")

        if os.path.exists(sadtalker_venv_python):
            python_exec = sadtalker_venv_python
            logger.info(f"ä½¿ç”¨ SadTalker è™šæ‹Ÿç¯å¢ƒ: {sadtalker_venv_python}")
        else:
            python_exec = "python"
            logger.warning("æœªæ£€æµ‹åˆ° SadTalker/.venvï¼Œä½¿ç”¨é»˜è®¤ Python")

        # æ„å»º SadTalker å‘½ä»¤
        cmd = [
            python_exec, "inference.py",
            "--driven_audio", audio_path,
            "--source_image", SADTALKER_IMAGE,
            "--result_dir", SADTALKER_OUTPUT_DIR,
            "--still",
            "--preprocess", "crop",
            # "--enhancer", "gfpgan",
            "--batch_size", "4"
        ]

        # åœ¨ SadTalker ç›®å½•ä¸‹æ‰§è¡Œ
        process = subprocess.Popen(
            cmd,
            cwd=SADTALKER_DIR,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        # è¯»å–è¾“å‡º
        output_lines = []
        for line in process.stdout:
            output_lines.append(line)
            logger.info(f"SadTalker: {line.strip()}")

        process.wait()

        if process.returncode != 0:
            logger.error(f"SadTalker æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
            return {"success": False, "error": "è§†é¢‘ç”Ÿæˆå¤±è´¥"}

        # æŸ¥æ‰¾ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆæœ€æ–°çš„ mp4 æ–‡ä»¶ï¼‰
        video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", "*.mp4")
        video_files = glob.glob(video_pattern, recursive=True)

        if not video_files:
            logger.error("æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶")
            return {"success": False, "error": "æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘"}

        # è·å–æœ€æ–°çš„è§†é¢‘æ–‡ä»¶
        latest_video = max(video_files, key=os.path.getmtime)
        video_filename = os.path.basename(latest_video)

        logger.info(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {latest_video}")
        return {
            "success": True,
            "video_path": latest_video,
            "video_filename": video_filename
        }

    except Exception as e:
        logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# ==================== åˆå§‹åŒ–ä»£ç†å®ä¾‹ ====================
agent = PsychologicalAgent(DEEPSEEK_API_KEY)


# ==================== API è·¯ç”± ====================

@app.route('/')
def root_redirect():
    """
    æ ¹è·¯å¾„ - é‡å®šå‘åˆ°é€‰æ‹©é¡µé¢
    """
    return redirect('/select')

@app.route('/index')
def main_page():
    """
    ä¸»é¡µé¢è·¯ç”± - æä¾›ä¸»é¡µé¢
    """
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return """
        <!DOCTYPE html>
        <html>
        <head><title>å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†</title></head>
        <body style="font-family: Arial; padding: 40px; text-align: center;">
            <h1 style="color: #4a90e2;">å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†</h1>
            <p>è¯·å…ˆ <a href="/select">é€‰æ‹©æ•°å­—äººå½¢è±¡</a></p>
        </body>
        </html>
        """


@app.route('/select')
def select_page():
    """é€‰æ‹©æ•°å­—äººå…¥å£é¡µ"""
    try:
        with open('select.html', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return """
        <!DOCTYPE html>
        <html>
        <head><title>é€‰æ‹©æ•°å­—äººå½¢è±¡</title></head>
        <body style="font-family: Arial; padding: 40px; text-align: center;">
            <h1 style="color: #4a90e2;">é€‰æ‹©æ•°å­—äººå½¢è±¡</h1>
            <p>select.html æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚</p>
        </body>
        </html>
        """, 404


@app.route('/avatars/<filename>')
def serve_avatar(filename):
    """
    æä¾›æ•°å­—äººå›¾ç‰‡æœåŠ¡
    """
    try:
        return send_from_directory('avatars', filename)
    except FileNotFoundError:
        logger.error(f"æ•°å­—äººå›¾ç‰‡æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å›¾ç‰‡ {filename} æœªæ‰¾åˆ°"}), 404


@app.route('/api/set_avatar', methods=['POST'])
def set_sadtalker_image():
    """
    è®¾ç½® SadTalker ä½¿ç”¨çš„æ•°å­—äººå›¾ç‰‡
    """
    try:
        data = request.get_json()
        avatar_id = data.get('avatar_id', '1')
        
        # æ ¹æ® avatar_id è®¾ç½®å¯¹åº”çš„å›¾ç‰‡
        avatar_images = {
            '1': 'avatar1.png',
            '2': 'avatar2.png', 
            '3': 'avatar3.png'
        }
        
        avatar_filename = avatar_images.get(avatar_id, 'avatar1.png')
        new_image_path = os.path.join('avatars', avatar_filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(new_image_path):
            # æ›´æ–° SadTalker é…ç½®ä¸­çš„å›¾ç‰‡è·¯å¾„
            global SADTALKER_IMAGE
            SADTALKER_IMAGE = new_image_path
            logger.info(f"å·²æ›´æ–° SadTalker å›¾ç‰‡ä¸º: {new_image_path}")
            
            # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¤åˆ¶åˆ° SadTalker ç›®å½•
            sadtalker_dest = os.path.join(SADTALKER_DIR, "my_photo.png")
            try:
                import shutil
                shutil.copy2(new_image_path, sadtalker_dest)
                logger.info(f"å·²å¤åˆ¶åˆ° SadTalker ç›®å½•: {sadtalker_dest}")
            except Exception as e:
                logger.warning(f"å¤åˆ¶åˆ° SadTalker ç›®å½•å¤±è´¥: {str(e)}")
            
            return jsonify({"success": True, "message": f"å·²åˆ‡æ¢ä¸ºå½¢è±¡ {avatar_id}"})
        else:
            logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {new_image_path}")
            return jsonify({"success": False, "error": "å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨"}), 404
            
    except Exception as e:
        logger.error(f"è®¾ç½®æ•°å­—äººå›¾ç‰‡å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


# ==================== è¯­éŸ³è¯†åˆ«API ====================
@app.route('/api/recognize_speech', methods=['POST'])
def recognize_speech():
    """
    è¯­éŸ³è¯†åˆ«æ¥å£
    
    æ”¯æŒä¸Šä¼ éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    try:
        logger.info("æ”¶åˆ°è¯­éŸ³è¯†åˆ«è¯·æ±‚")
        
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†è¯­éŸ³è¯†åˆ«åº“
        if not SPEECH_RECOGNITION_AVAILABLE:
            return jsonify({
                "success": False,
                "error": "è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·å®‰è£…ä¾èµ–: pip install SpeechRecognition pydub"
            }), 501
        
        # æ£€æŸ¥è¯·æ±‚æ•°æ®
        if 'audio' not in request.files and 'audio_data' not in request.form:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›éŸ³é¢‘æ•°æ®"}), 400
        
        audio_format = request.form.get('format', 'webm')
        
        if 'audio' in request.files:
            # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
            audio_file = request.files['audio']
            audio_data = audio_file.read()
        else:
            # å¤„ç†Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
            audio_data_str = request.form.get('audio_data', '')
            if ',' in audio_data_str:
                audio_data_str = audio_data_str.split(',')[1]
            audio_data = base64.b64decode(audio_data_str)
        
        # è¯†åˆ«è¯­éŸ³
        result = recognize_speech_from_audio(audio_data, audio_format)
        
        if result["success"]:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            filename = f"speech_{uuid.uuid4().hex[:8]}.{audio_format}"
            save_audio_file(audio_data, filename)
            
            result["audio_url"] = f"/api/speech/{filename}"
            result["timestamp"] = datetime.datetime.now().isoformat()
            
            logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {result['text'][:50]}...")
        else:
            logger.warning(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«æ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}",
            "text": ""
        }), 500


@app.route('/api/speech/<filename>')
def serve_speech(filename):
    """
    æä¾›è¯­éŸ³æ–‡ä»¶æœåŠ¡
    """
    try:
        return send_from_directory(SPEECH_INPUT_DIR, filename)
    except FileNotFoundError:
        logger.error(f"è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": "è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """
    å¿ƒç†åˆ†æä¸»æ¥å£ï¼ˆé›†æˆ TTS å’Œæ•°å­—äººè§†é¢‘ç”Ÿæˆï¼‰

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼‰
        - generate_video: æ˜¯å¦ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯é€‰ï¼Œé»˜è®¤ Trueï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«è§†é¢‘ URL
    """
    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')
        generate_video = data.get('generate_video', True)

        logger.info(f"æ”¶åˆ°åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}, ç”Ÿæˆè§†é¢‘: {generate_video}")

        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # è°ƒç”¨å¿ƒç†åˆ†æä»£ç†
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            result["detected_emotion"] = detected_emotion
            result["timestamp"] = datetime.datetime.now().isoformat()

            # å¦‚æœéœ€è¦ç”Ÿæˆè§†é¢‘
            if generate_video:
                response_text = result.get("response", "")

                # æ¸…ç† HTML æ ‡ç­¾ï¼Œåªä¿ç•™çº¯æ–‡æœ¬ç”¨äº TTS
                import re
                clean_text = re.sub(r'<[^>]+>', '', response_text)
                clean_text = clean_text.replace('<br>', 'ã€‚').replace('&nbsp;', ' ')

                # æ­¥éª¤1: TTS æ–‡å­—è½¬è¯­éŸ³
                logger.info("å¼€å§‹ TTS è½¬æ¢...")
                tts_result = text_to_speech(clean_text)

                if tts_result["success"]:
                    audio_path = tts_result["audio_path"]
                    result["audio_url"] = f"/api/audio/{tts_result['audio_filename']}"

                    # æ­¥éª¤2: SadTalker ç”Ÿæˆè§†é¢‘
                    logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
                    video_result = generate_talking_video(audio_path)

                    if video_result["success"]:
                        result["video_url"] = f"/api/video/{video_result['video_filename']}"
                        result["video_generated"] = True
                        logger.info(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {video_result['video_filename']}")
                    else:
                        result["video_generated"] = False
                        result["video_error"] = video_result.get("error", "è§†é¢‘ç”Ÿæˆå¤±è´¥")
                        logger.warning(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {video_result.get('error')}")
                else:
                    result["video_generated"] = False
                    result["tts_error"] = tts_result.get("error", "TTS è½¬æ¢å¤±è´¥")
                    logger.warning(f"TTS è½¬æ¢å¤±è´¥: {tts_result.get('error')}")

        return jsonify(result)

    except Exception as e:
        logger.error(f"åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/video/<filename>')
def serve_video(filename):
    """
    æä¾›è§†é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: è§†é¢‘æ–‡ä»¶å

    Returns:
        è§†é¢‘æ–‡ä»¶
    """
    # åœ¨ SadTalker è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘
    video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", filename)
    video_files = glob.glob(video_pattern, recursive=True)

    if video_files:
        video_path = video_files[0]
        directory = os.path.dirname(video_path)
        return send_from_directory(directory, filename, mimetype='video/mp4')

    logger.error(f"è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/audio/<filename>')
def serve_audio(filename):
    """
    æä¾›éŸ³é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: éŸ³é¢‘æ–‡ä»¶å

    Returns:
        éŸ³é¢‘æ–‡ä»¶
    """
    audio_path = os.path.join(AUDIO_OUTPUT_DIR, filename)

    if os.path.exists(audio_path):
        return send_from_directory(AUDIO_OUTPUT_DIR, filename, mimetype='audio/mpeg')

    logger.error(f"éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze_local', methods=['POST'])
def analyze_local():
    """
    æœ¬åœ°æ¨¡å‹åˆ†ææ¥å£ï¼ˆå½“å‰ä½¿ç”¨ DeepSeek API ä½œä¸ºåå¤‡ï¼‰

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')

        logger.info(f"æ”¶åˆ°æœ¬åœ°åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}")

        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # å°è¯•ä½¿ç”¨ DeepSeek API
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            result["detected_emotion"] = detected_emotion
            result["timestamp"] = datetime.datetime.now().isoformat()
        else:
            # API å¤±è´¥æ—¶ä½¿ç”¨å¤‡é€‰å›å¤
            result = generate_fallback_response(user_input, detected_emotion)

        return jsonify(result)

    except Exception as e:
        logger.error(f"æœ¬åœ°åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/analyze_emotion', methods=['POST'])
def analyze_emotion():
    """
    è¡¨æƒ…è¯†åˆ«æ¥å£

    è¯·æ±‚ä½“:
        - image: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        JSON æ ¼å¼çš„æƒ…ç»ªåˆ†æç»“æœ
    """
    try:
        data = request.get_json()

        if not data or 'image' not in data:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›å›¾ç‰‡æ•°æ®"}), 400

        logger.info("æ”¶åˆ°è¡¨æƒ…åˆ†æè¯·æ±‚")

        # åˆ†æè¡¨æƒ…
        result = analyze_emotion_from_image(data['image'])

        return jsonify({
            "success": True,
            "dominant_emotion": result["dominant_emotion"],
            "emotion_scores": result["emotion_scores"],
            "face_detected": result["face_detected"],
            "timestamp": datetime.datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"è¡¨æƒ…åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }), 500


@app.route('/api/model/status', methods=['GET'])
def model_status():
    """
    æ¨¡å‹çŠ¶æ€æŸ¥è¯¢æ¥å£

    Returns:
        JSON æ ¼å¼çš„æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    """
    return jsonify({
        "local_model_loaded": False,
        "model_loading": False,
        "deepface_available": DEEPFACE_AVAILABLE,
        "deepseek_api_available": True,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/status', methods=['GET'])
def api_status():
    """
    API çŠ¶æ€æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„ API çŠ¶æ€ä¿¡æ¯
    """
    api_test = test_deepseek_api()

    return jsonify({
        "status": "healthy" if api_test.get("success") else "warning",
        "deepseek_api": api_test,
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„æœåŠ¡å¥åº·çŠ¶æ€
    """
    return jsonify({
        "status": "healthy",
        "service": "å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†",
        "version": "2.2",
        "timestamp": datetime.datetime.now().isoformat(),
        "features": {
            "psychological_analysis": True,
            "emotion_recognition": DEEPFACE_AVAILABLE,
            "real_time_camera": True,
            "deepseek_api": True,
            "avatar_selection": True,
            "speech_input": SPEECH_RECOGNITION_AVAILABLE
        }
    })


@app.route('/api/conversation/summary', methods=['GET'])
def get_conversation_summary():
    """
    è·å–å¯¹è¯æ‘˜è¦

    Returns:
        JSON æ ¼å¼çš„å¯¹è¯æ‘˜è¦ä¿¡æ¯
    """
    user_messages = [msg['content'] for msg in conversation_history if msg['role'] == 'user']

    return jsonify({
        "total_conversations": len(conversation_history) // 2,
        "recent_topics": user_messages[-3:] if user_messages else [],
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/conversation/reset', methods=['POST'])
def reset_conversation():
    """
    é‡ç½®å¯¹è¯å†å²

    Returns:
        JSON æ ¼å¼çš„æ“ä½œç»“æœ
    """
    global conversation_history
    conversation_history = []
    logger.info("å¯¹è¯å†å²å·²é‡ç½®")
    return jsonify({"success": True, "message": "å¯¹è¯å·²é‡ç½®"})


@app.route('/api/debug', methods=['GET'])
def debug_info():
    """
    è°ƒè¯•ä¿¡æ¯æ¥å£ï¼ˆä»…ç”¨äºå¼€å‘ï¼‰

    Returns:
        JSON æ ¼å¼çš„è°ƒè¯•ä¿¡æ¯
    """
    return jsonify({
        "routes": [str(rule) for rule in app.url_map.iter_rules()],
        "conversation_length": len(conversation_history),
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "api_key_set": bool(DEEPSEEK_API_KEY),
        "sadtalker_image": SADTALKER_IMAGE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/favicon.ico')
def favicon():
    """å¤„ç† favicon è¯·æ±‚ï¼Œé¿å… 404 é”™è¯¯"""
    return '', 204


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† v2.2")
    print("=" * 60)
    print(f"ğŸ“± æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"ğŸ‘¤ å½¢è±¡é€‰æ‹©: http://localhost:5000/select")
    print(f"ğŸ’¬ ä¸»é¡µé¢: http://localhost:5000/index")
    print(f"ğŸ¤ è¯­éŸ³è¾“å…¥: æ”¯æŒï¼ˆéœ€è¦æµè§ˆå™¨æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼‰")
    print(f"â¤ï¸  å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")
    print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€: http://localhost:5000/api/model/status")
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: http://localhost:5000/api/debug")
    print("=" * 60)
    print("å¯ç”¨ API ç«¯ç‚¹:")
    print("  POST /api/analyze        - å¿ƒç†åˆ†æ")
    print("  POST /api/analyze_local  - æœ¬åœ°æ¨¡å‹åˆ†æ")
    print("  POST /api/analyze_emotion - è¡¨æƒ…è¯†åˆ«")
    print("  POST /api/set_avatar     - è®¾ç½®æ•°å­—äººå½¢è±¡")
    print("  POST /api/recognize_speech - è¯­éŸ³è¯†åˆ«")
    print("  GET  /api/health         - å¥åº·æ£€æŸ¥")
    print("  GET  /api/model/status   - æ¨¡å‹çŠ¶æ€")
    print("  GET  /api/conversation/summary - å¯¹è¯æ‘˜è¦")
    print("  POST /api/conversation/reset   - é‡ç½®å¯¹è¯")
    print("=" * 60)
    
    # æ£€æŸ¥è¯­éŸ³è¯†åˆ«åŠŸèƒ½
    if SPEECH_RECOGNITION_AVAILABLE:
        print("âœ… è¯­éŸ³è¯†åˆ«åŠŸèƒ½å·²å¯ç”¨")
    else:
        print("âš ï¸  è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install SpeechRecognition pydub")
    
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)

    # å¯åŠ¨ Flask æœåŠ¡
    app.run(
        debug=True,
        host='0.0.0.0',
        port=5000,
        use_reloader=False  # ç¦ç”¨è‡ªåŠ¨é‡è½½ï¼Œé¿å…æ¨¡å‹é‡å¤åŠ è½½
    )