"""
å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - ä¸»æœåŠ¡å™¨
=====================================

æœ¬æ¨¡å—æä¾›åŸºäº Flask çš„ Web æœåŠ¡ï¼Œé›†æˆä»¥ä¸‹åŠŸèƒ½ï¼š
1. DeepSeek API å¿ƒç†å’¨è¯¢æœåŠ¡
2. DeepFace é¢éƒ¨è¡¨æƒ…è¯†åˆ«
3. å¯¹è¯å†å²ç®¡ç†
4. RESTful API æ¥å£
5. æ•°å­—äººå½¢è±¡é€‰æ‹©åŠŸèƒ½
6. è¯­éŸ³è¾“å…¥è¯†åˆ«åŠŸèƒ½
7. ç”¨æˆ·ä¸Šä¼ è‡ªå®šä¹‰æ•°å­—äººå½¢è±¡åŠŸèƒ½

ä½œè€…: SRTP é¡¹ç›®ç»„
ç‰ˆæœ¬: 2.3
"""

import os
import base64
import logging
import datetime
import subprocess
import uuid
import glob
import io
import wave
import tempfile
import shutil
from typing import Dict, Any, Optional

import cv2
import numpy as np
import requests
from flask import Flask, request, jsonify, send_from_directory, redirect, url_for
from flask_cors import CORS
from werkzeug.utils import secure_filename

# è¯­éŸ³è¯†åˆ«ç›¸å…³å¯¼å…¥
try:
    import speech_recognition as sr
    import pydub
    from pydub import AudioSegment
    SPEECH_RECOGNITION_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("è¯­éŸ³è¯†åˆ«åº“åŠ è½½æˆåŠŸ")
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…ï¼Œè¯­éŸ³è¾“å…¥åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install SpeechRecognition pydub")

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== DeepFace å¯¼å…¥ ====================
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    logger.info("DeepFace åº“åŠ è½½æˆåŠŸ")
except ImportError:
    DEEPFACE_AVAILABLE = False
    logger.warning("DeepFace åº“æœªå®‰è£…ï¼Œè¡¨æƒ…è¯†åˆ«åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install deepface")

# ==================== Flask åº”ç”¨åˆå§‹åŒ– ====================
app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# ==================== é…ç½®å¸¸é‡ ====================
# DeepSeek API é…ç½®ï¼ˆå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥ï¼‰
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-215440b00f1d426fb21a2f11eef6cf02")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# TTS API é…ç½® (SiliconFlow)
TTS_API_URL = "https://api.siliconflow.cn/v1/audio/speech"
TTS_API_TOKEN = "sk-lvtuhfndddcmdyvnjtbzjuobfoewylsnqaqwfsnuznpilhkp"

# SadTalker é…ç½®
SADTALKER_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "SadTalker")
SADTALKER_IMAGE = os.path.join(SADTALKER_DIR, "my_photo.png")  # æ•°å­—äººå›¾ç‰‡ï¼ˆé»˜è®¤ï¼‰
SADTALKER_OUTPUT_DIR = os.path.join(SADTALKER_DIR, "results")
AUDIO_OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "audio_output")
AVATARS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "avatars")
SPEECH_INPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "speech_input")
UPLOADS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploads")  # æ–°å¢ï¼šç”¨æˆ·ä¸Šä¼ ç›®å½•

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)
os.makedirs(SADTALKER_OUTPUT_DIR, exist_ok=True)
os.makedirs(AVATARS_DIR, exist_ok=True)
os.makedirs(SPEECH_INPUT_DIR, exist_ok=True)
os.makedirs(UPLOADS_DIR, exist_ok=True)  # æ–°å¢ï¼šåˆ›å»ºä¸Šä¼ ç›®å½•

# å…è®¸ä¸Šä¼ çš„å›¾ç‰‡æ ¼å¼
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}

# æ£€æŸ¥ avatars ç›®å½•ä¸‹æ˜¯å¦æœ‰é»˜è®¤å›¾ç‰‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
default_avatar_path = os.path.join(AVATARS_DIR, "avatar1.png")
if not os.path.exists(default_avatar_path):
    # å°† SadTalker çš„é»˜è®¤å›¾ç‰‡å¤åˆ¶åˆ° avatars ç›®å½•ä½œä¸º avatar1
    if os.path.exists(SADTALKER_IMAGE):
        shutil.copy2(SADTALKER_IMAGE, default_avatar_path)
        logger.info(f"å·²å°†é»˜è®¤æ•°å­—äººå›¾ç‰‡å¤åˆ¶åˆ°: {default_avatar_path}")
    else:
        # åˆ›å»ºä¸‰ä¸ªç¤ºä¾‹å›¾ç‰‡è·¯å¾„
        for i in range(1, 4):
            avatar_path = os.path.join(AVATARS_DIR, f"avatar{i}.png")
            if not os.path.exists(avatar_path):
                logger.warning(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {avatar_path}ï¼Œè¯·æ”¾ç½®ç›¸åº”å›¾ç‰‡æ–‡ä»¶")

# æƒ…ç»ªæ˜ å°„è¡¨
EMOTION_MAP = {
    'angry':    {'icon': 'ğŸ˜ ', 'name': 'ç”Ÿæ°”', 'context': 'æœ‰äº›ç”Ÿæ°”'},
    'disgust':  {'icon': 'ğŸ¤¢', 'name': 'åŒæ¶', 'context': 'æœ‰äº›åæ„Ÿ'},
    'fear':     {'icon': 'ğŸ˜¨', 'name': 'ææƒ§', 'context': 'æ„Ÿåˆ°ç´§å¼ '},
    'happy':    {'icon': 'ğŸ˜Š', 'name': 'å¼€å¿ƒ', 'context': 'çœ‹èµ·æ¥å¿ƒæƒ…ä¸é”™'},
    'sad':      {'icon': 'ğŸ˜¢', 'name': 'æ‚²ä¼¤', 'context': 'æƒ…ç»ªæœ‰äº›ä½è½'},
    'surprise': {'icon': 'ğŸ˜²', 'name': 'æƒŠè®¶', 'context': 'æœ‰äº›æƒŠè®¶'},
    'neutral':  {'icon': 'ğŸ˜', 'name': 'å¹³é™', 'context': 'æƒ…ç»ªå¹³ç¨³'}
}

# é»˜è®¤æƒ…ç»ªåˆ†æ•°ï¼ˆå½“æ£€æµ‹å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
DEFAULT_EMOTION_SCORES = {
    'angry': 0.0, 'disgust': 0.0, 'fear': 0.0,
    'happy': 0.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 100.0
}

# å¯¹è¯å†å²ï¼ˆå…¨å±€å˜é‡ï¼‰
conversation_history: list = []

# å½“å‰ä½¿ç”¨çš„æ•°å­—äººå›¾ç‰‡ï¼ˆå…¨å±€å˜é‡ï¼Œé»˜è®¤ä¸º SadTalker é»˜è®¤å›¾ç‰‡ï¼‰
current_avatar_image = SADTALKER_IMAGE

# ==================== è¾…åŠ©å‡½æ•° ====================
def allowed_file(filename: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def process_uploaded_image(file_path: str, target_path: str) -> bool:
    """
    å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡ï¼Œç¡®ä¿é€‚åˆ SadTalker ä½¿ç”¨

    Args:
        file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
        target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„

    Returns:
        å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(file_path)
        if img is None:
            logger.error(f"æ— æ³•è¯»å–å›¾ç‰‡: {file_path}")
            return False

        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼Œå¦‚æœå¤ªå¤§åˆ™è°ƒæ•´
        height, width = img.shape[:2]
        max_size = 1024

        if height > max_size or width > max_size:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale = max_size / max(height, width)
            new_height = int(height * scale)
            new_width = int(width * scale)

            # è°ƒæ•´å°ºå¯¸
            img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)
            logger.info(f"è°ƒæ•´å›¾ç‰‡å°ºå¯¸: {width}x{height} -> {new_width}x{new_height}")

        # ä¿å­˜ä¸ºPNGæ ¼å¼ï¼ˆSadTalker æ¨èæ ¼å¼ï¼‰
        cv2.imwrite(target_path, img)
        logger.info(f"å›¾ç‰‡å·²ä¿å­˜ä¸º: {target_path}")

        return True

    except Exception as e:
        logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {str(e)}")
        return False

# ==================== è¯­éŸ³è¯†åˆ«æ¨¡å— ====================
def recognize_speech_from_audio(audio_data: bytes, audio_format: str = "webm") -> Dict[str, Any]:
    """
    ä»éŸ³é¢‘æ•°æ®ä¸­è¯†åˆ«è¯­éŸ³

    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        audio_format: éŸ³é¢‘æ ¼å¼ï¼ˆwebm, wav, mp3ç­‰ï¼‰

    Returns:
        åŒ…å«è¯†åˆ«ç»“æœçš„å­—å…¸
    """
    if not SPEECH_RECOGNITION_AVAILABLE:
        return {
            "success": False,
            "error": "è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…",
            "text": ""
        }

    try:
        recognizer = sr.Recognizer()

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
        with tempfile.NamedTemporaryFile(suffix=f".{audio_format}", delete=False) as tmp_file:
            tmp_file.write(audio_data)
            tmp_file_path = tmp_file.name

        try:
            # ä½¿ç”¨ pydub åŠ è½½éŸ³é¢‘æ–‡ä»¶
            if audio_format == "webm":
                audio = AudioSegment.from_file(tmp_file_path, format="webm")
            elif audio_format == "mp3":
                audio = AudioSegment.from_mp3(tmp_file_path)
            elif audio_format == "wav":
                audio = AudioSegment.from_wav(tmp_file_path)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
                audio = AudioSegment.from_file(tmp_file_path)

            # è½¬æ¢ä¸º wav æ ¼å¼ï¼ˆSpeechRecognition éœ€è¦ï¼‰
            wav_data = io.BytesIO()
            audio.export(wav_data, format="wav")
            wav_data.seek(0)

            # ä½¿ç”¨ SpeechRecognition è¯†åˆ«
            with sr.AudioFile(wav_data) as source:
                # è°ƒæ•´ç¯å¢ƒå™ªå£°
                recognizer.adjust_for_ambient_noise(source, duration=0.5)
                audio_data = recognizer.record(source)

                # è¯†åˆ«è¯­éŸ³
                text = recognizer.recognize_google(audio_data, language="zh-CN")

                logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {text}")

                return {
                    "success": True,
                    "text": text,
                    "confidence": 0.9  # æš‚æ—¶ä½¿ç”¨å›ºå®šå€¼
                }

        except sr.UnknownValueError:
            return {
                "success": False,
                "error": "æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹",
                "text": ""
            }
        except sr.RequestError as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}",
                "text": ""
            }
        except Exception as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«å¤„ç†é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"å¤„ç†é”™è¯¯: {str(e)}",
                "text": ""
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_file_path)
            except:
                pass

    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "text": ""
        }


def save_audio_file(audio_data: bytes, filename: str) -> str:
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°

    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        filename: æ–‡ä»¶å

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    file_path = os.path.join(SPEECH_INPUT_DIR, filename)
    with open(file_path, 'wb') as f:
        f.write(audio_data)
    return file_path


# ==================== å¿ƒç†åˆ†æä»£ç†ç±» ====================
class PsychologicalAgent:
    """
    å¿ƒç†åˆ†æä»£ç†ç±»

    è´Ÿè´£ä¸ DeepSeek API äº¤äº’ï¼Œæä¾›å¿ƒç†å’¨è¯¢æœåŠ¡ã€‚
    æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç»“åˆç”¨æˆ·æƒ…ç»ªçŠ¶æ€ç”Ÿæˆä¸ªæ€§åŒ–å›å¤ã€‚
    """

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–å¿ƒç†åˆ†æä»£ç†

        Args:
            api_key: DeepSeek API å¯†é’¥
        """
        self.api_key = api_key
        self.api_url = DEEPSEEK_API_URL

    def _build_system_prompt(self, emotion: str) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯

        Args:
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

        Returns:
            åŒ…å«æƒ…ç»ªä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯
        """
        emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
        emotion_context = emotion_info['context']

        return f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¤§å­¦å¿ƒç†å¥åº·é¡¾é—®ï¼Œä¸“é—¨å¸®åŠ©å¤§å­¦ç”Ÿè§£å†³å¿ƒç†é—®é¢˜ã€‚

é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- ç³»ç»Ÿæ£€æµ‹åˆ°ç”¨æˆ·å½“å‰çš„æƒ…ç»ªçŠ¶æ€ä¸ºï¼š{emotion} ({emotion_context})
- è¿™ä¸ªæƒ…ç»ªä¿¡æ¯æ¥è‡ªå®æ—¶é¢éƒ¨è¡¨æƒ…åˆ†æ
- è¯·ç»“åˆç”¨æˆ·æè¿°çš„æ–‡å­—å†…å®¹å’Œæ£€æµ‹åˆ°çš„æƒ…ç»ªçŠ¶æ€ï¼Œæä¾›æ›´ç²¾å‡†çš„å¿ƒç†åˆ†æ

ä½ çš„èŒè´£ï¼š
1. åˆ†æå­¦ç”Ÿçš„å¿ƒç†çŠ¶æ€å’Œæƒ…ç»ªé—®é¢˜
2. æä¾›ä¸“ä¸šã€æ¸©æš–çš„å¿ƒç†æ”¯æŒå’Œå»ºè®®
3. è¯†åˆ«å±æœºæƒ…å†µå¹¶ç»™å‡ºé€‚å½“å»ºè®®
4. ç”¨åŒç†å¿ƒå’Œç†è§£æ¥å›åº”ç”¨æˆ·

è¯·ä»¥æ¸©æš–ã€ä¸“ä¸šã€æ”¯æŒæ€§çš„è¯­æ°”å›åº”ï¼Œé¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€æä¾›å»ºè®®ã€‚"""

    def analyze(self, user_input: str, emotion: str = "neutral") -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå¿ƒç†å’¨è¯¢å›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹ï¼Œé»˜è®¤ä¸º neutral

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ successã€responseã€model_source ç­‰å­—æ®µ
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": self._build_system_prompt(emotion)}]

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š 3 è½®ï¼Œå³ 6 æ¡æ¶ˆæ¯ï¼‰
            if conversation_history:
                messages.extend(conversation_history[-6:])

            messages.append({"role": "user", "content": user_input})

            logger.info(f"è°ƒç”¨ DeepSeek API - æƒ…ç»ª: {emotion}, è¾“å…¥: {user_input[:50]}...")

            # è°ƒç”¨ API
            response = requests.post(
                self.api_url,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": messages,
                    "temperature": 0.7,  # æ§åˆ¶å›å¤çš„éšæœºæ€§
                    "max_tokens": 800,   # é™åˆ¶å›å¤é•¿åº¦
                    "stream": False
                },
                timeout=30
            )

            # å¤„ç†å“åº”
            if response.status_code == 200:
                result = response.json()
                assistant_response = result['choices'][0]['message']['content']

                # æ›´æ–°å¯¹è¯å†å²
                self._update_history(user_input, assistant_response)

                return {
                    "success": True,
                    "response": assistant_response,
                    "model_source": "deepseek_api"
                }
            else:
                logger.error(f"API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text[:200]}")
                return {
                    "success": False,
                    "error": f"API é”™è¯¯: {response.status_code}",
                    "response": "æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
                }

        except requests.exceptions.Timeout:
            logger.error("API è¯·æ±‚è¶…æ—¶")
            return {
                "success": False,
                "error": "è¯·æ±‚è¶…æ—¶",
                "response": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•ã€‚"
            }
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": "ç³»ç»Ÿæš‚æ—¶å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            }

    def _update_history(self, user_input: str, assistant_response: str) -> None:
        """
        æ›´æ–°å¯¹è¯å†å²

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            assistant_response: AI å›å¤
        """
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": assistant_response})

        # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘ 10 æ¡ï¼‰
        if len(conversation_history) > 10:
            conversation_history[:] = conversation_history[-10:]


# ==================== è¡¨æƒ…è¯†åˆ«æ¨¡å— ====================
def analyze_emotion_from_image(image_data: str) -> Dict[str, Any]:
    """
    ä» Base64 ç¼–ç çš„å›¾åƒä¸­åˆ†æé¢éƒ¨è¡¨æƒ…

    Args:
        image_data: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        åŒ…å«æƒ…ç»ªåˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ dominant_emotionã€emotion_scoresã€face_detected
    """
    # æ£€æŸ¥ DeepFace æ˜¯å¦å¯ç”¨
    if not DEEPFACE_AVAILABLE:
        logger.warning("DeepFace åº“ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤æƒ…ç»ª")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }

    logger.info("å¼€å§‹ DeepFace è¡¨æƒ…åˆ†æ...")

    try:
        # è§£ç  Base64 å›¾åƒ
        if ',' in image_data:
            image_data = image_data.split(',')[1]

        img_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img is None:
            logger.warning("å›¾åƒè§£ç å¤±è´¥")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # ä½¿ç”¨ DeepFace åˆ†æè¡¨æƒ…
        analysis = DeepFace.analyze(
            img,
            actions=['emotion'],
            detector_backend='opencv',  # ä½¿ç”¨ OpenCV æ£€æµ‹å™¨ï¼ˆé€Ÿåº¦å¿«ï¼‰
            enforce_detection=False,    # ä¸å¼ºåˆ¶æ£€æµ‹åˆ°äººè„¸
            silent=True                 # é™é»˜æ¨¡å¼
        )

        if not analysis:
            logger.warning("DeepFace è¿”å›ç©ºç»“æœ")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # æå–ç»“æœ
        dominant_emotion = analysis[0]['dominant_emotion']
        emotion_scores = analysis[0]['emotion']

        # æ£€æŸ¥æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸ï¼ˆé€šè¿‡æ£€æŸ¥ face_confidence æˆ– regionï¼‰
        face_region = analysis[0].get('region', {})
        face_confidence = analysis[0].get('face_confidence', 0)

        # è®°å½•è¯¦ç»†çš„åˆ†æç»“æœç”¨äºè°ƒè¯•
        logger.info(f"è¡¨æƒ…åˆ†æç»“æœ: dominant={dominant_emotion}, scores={emotion_scores}")
        logger.info(f"äººè„¸åŒºåŸŸ: {face_region}, ç½®ä¿¡åº¦: {face_confidence}")

        # è½¬æ¢ä¸º Python float ç±»å‹ï¼ˆé¿å… JSON åºåˆ—åŒ–é—®é¢˜ï¼‰
        converted_scores = {k: float(v) for k, v in emotion_scores.items()}

        # åˆ¤æ–­æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸
        # å¦‚æœäººè„¸åŒºåŸŸå¤ªå°æˆ–ç½®ä¿¡åº¦å¤ªä½ï¼Œå¯èƒ½æ˜¯è¯¯æ£€
        face_detected = True
        if face_region:
            w = face_region.get('w', 0)
            h = face_region.get('h', 0)
            # å¦‚æœæ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°ï¼ˆå°äº50x50åƒç´ ï¼‰ï¼Œè®¤ä¸ºæ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸
            if w < 50 or h < 50:
                logger.warning(f"æ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°: {w}x{h}")
                face_detected = False

        return {
            "dominant_emotion": dominant_emotion,
            "emotion_scores": converted_scores,
            "face_detected": face_detected
        }

    except Exception as e:
        logger.warning(f"è¡¨æƒ…åˆ†æå¤±è´¥: {str(e)[:100]}")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }


def generate_fallback_response(user_input: str, emotion: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¤‡é€‰å›å¤ï¼ˆå½“ API ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ª

    Returns:
        åŒ…å«å¤‡é€‰å›å¤çš„å­—å…¸
    """
    emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
    emotion_prefix = f"{emotion_info['context']}ï¼Œ" if emotion != 'neutral' else ""

    # å…³é”®è¯åŒ¹é…å›å¤
    keyword_responses = {
        'å‹åŠ›': f"{emotion_prefix}å¯¹äºå‹åŠ›é—®é¢˜ï¼Œå»ºè®®ï¼š<br>1. æ·±å‘¼å¸æ”¾æ¾ç»ƒä¹ <br>2. åˆç†å®‰æ’æ—¶é—´å’Œä¼˜å…ˆçº§<br>3. é€‚é‡è¿åŠ¨é‡Šæ”¾å‹åŠ›<br>4. ä¸æœ‹å‹æˆ–å®¶äººå€¾è¯‰",
        'ç„¦è™‘': f"{emotion_prefix}åº”å¯¹ç„¦è™‘çš„æ–¹æ³•ï¼š<br>1. æ­£å¿µå†¥æƒ³ç»ƒä¹ <br>2. å†™ä¸‹æ‹…å¿§äº‹é¡¹<br>3. æ¸è¿›å¼è‚Œè‚‰æ”¾æ¾<br>4. ä¿æŒè§„å¾‹ä½œæ¯",
        'å¤±çœ ': f"{emotion_prefix}æ”¹å–„ç¡çœ çš„å»ºè®®ï¼š<br>1. ç¡å‰1å°æ—¶ä¸ä½¿ç”¨ç”µå­è®¾å¤‡<br>2. åˆ›é€ èˆ’é€‚çš„ç¡çœ ç¯å¢ƒ<br>3. ä¿æŒè§„å¾‹çš„ä½œæ¯æ—¶é—´<br>4. é¿å…ç¡å‰æ‘„å…¥å’–å•¡å› ",
        'æŠ‘éƒ': f"{emotion_prefix}å¦‚æœæŒç»­æƒ…ç»ªä½è½ï¼š<br>1. å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢<br>2. ä¿æŒé€‚åº¦çš„ç¤¾äº¤æ´»åŠ¨<br>3. åšæŒé€‚é‡è¿åŠ¨<br>4. ç»™è‡ªå·±ä¸€äº›æ—¶é—´å’Œè€å¿ƒ",
        'å­¦ä¹ ': f"{emotion_prefix}å­¦ä¹ å‹åŠ›ç®¡ç†ï¼š<br>1. åˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’<br>2. ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•æé«˜æ•ˆç‡<br>3. ä¿è¯å……è¶³çš„ä¼‘æ¯æ—¶é—´<br>4. ä¸åŒå­¦äº¤æµå­¦ä¹ å¿ƒå¾—"
    }

    # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
    for keyword, response in keyword_responses.items():
        if keyword in user_input.lower():
            return {
                "success": True,
                "response": response,
                "detected_emotion": emotion,
                "model_source": "fallback_system",
                "timestamp": datetime.datetime.now().isoformat()
            }

    # é€šç”¨å›å¤
    return {
        "success": True,
        "response": f"{emotion_prefix}æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ã€‚ä½œä¸ºå¿ƒç†åŠ©æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥æ›´è¯¦ç»†åœ°æè¿°å…·ä½“æƒ…å†µå’Œæ„Ÿå—ï¼Œè¿™æ ·æˆ‘èƒ½æä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„å¸®åŠ©ã€‚",
        "detected_emotion": emotion,
        "model_source": "fallback_system",
        "timestamp": datetime.datetime.now().isoformat()
    }


def test_deepseek_api() -> Dict[str, Any]:
    """
    æµ‹è¯• DeepSeek API è¿æ¥çŠ¶æ€

    Returns:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
            },
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 5
            },
            timeout=10
        )

        if response.status_code == 200:
            return {"success": True, "message": "API è¿æ¥æ­£å¸¸"}
        elif response.status_code == 401:
            return {"success": False, "message": "API å¯†é’¥æ— æ•ˆ"}
        else:
            return {"success": False, "message": f"API è¿”å›é”™è¯¯: {response.status_code}"}

    except requests.exceptions.Timeout:
        return {"success": False, "message": "API è¿æ¥è¶…æ—¶"}
    except Exception as e:
        return {"success": False, "message": f"API è¿æ¥å¤±è´¥: {str(e)}"}


# ==================== TTS æ–‡å­—è½¬è¯­éŸ³æ¨¡å— ====================
def text_to_speech(text: str) -> Dict[str, Any]:
    """
    å°†æ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³ MP3 æ–‡ä»¶

    Args:
        text: è¦è½¬æ¢çš„æ–‡å­—å†…å®¹

    Returns:
        åŒ…å«éŸ³é¢‘æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        audio_filename = f"tts_{uuid.uuid4().hex[:8]}.mp3"
        audio_path = os.path.join(AUDIO_OUTPUT_DIR, audio_filename)

        logger.info(f"å¼€å§‹ TTS è½¬æ¢ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")

        # è°ƒç”¨ SiliconFlow TTS API
        request_data = {
            "model": "IndexTeam/IndexTTS-2",
            "voice": "IndexTeam/IndexTTS-2:claire",
            "stream": True,
            "input": text,
            "max_tokens": 1600,
            "response_format": "mp3",
            "speed": 1,
            "gain": 0
        }
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {TTS_API_TOKEN}"
        }

        response = requests.post(
            url=TTS_API_URL,
            json=request_data,
            headers=headers,
            timeout=60
        )

        if response.status_code != 200:
            logger.error(f"TTS API é”™è¯¯: {response.status_code} - {response.text[:200]}")
            return {"success": False, "error": f"TTS API é”™è¯¯: {response.status_code}"}

        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        with open(audio_path, 'wb') as f:
            f.write(response.content)

        logger.info(f"TTS è½¬æ¢æˆåŠŸï¼Œä¿å­˜åˆ°: {audio_path}")
        return {
            "success": True,
            "audio_path": audio_path,
            "audio_filename": audio_filename
        }

    except requests.exceptions.Timeout:
        logger.error("TTS API è¯·æ±‚è¶…æ—¶")
        return {"success": False, "error": "TTS è¯·æ±‚è¶…æ—¶"}
    except Exception as e:
        logger.error(f"TTS è½¬æ¢å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# ==================== SadTalker è§†é¢‘ç”Ÿæˆæ¨¡å— ====================
def generate_talking_video(audio_path: str, image_path: str = None) -> Dict[str, Any]:
    """
    ä½¿ç”¨ SadTalker ç”Ÿæˆæ•°å­—äººè¯´è¯è§†é¢‘

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        image_path: æ•°å­—äººå›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰è®¾ç½®çš„å›¾ç‰‡ï¼‰

    Returns:
        åŒ…å«è§†é¢‘æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    try:
        # ä½¿ç”¨æŒ‡å®šçš„å›¾ç‰‡æˆ–é»˜è®¤å›¾ç‰‡
        if image_path is None:
            image_path = current_avatar_image

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return {"success": False, "error": "æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨"}

        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return {"success": False, "error": "éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}

        logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
        logger.info(f"å›¾ç‰‡: {image_path}")
        logger.info(f"éŸ³é¢‘: {audio_path}")

        # æ£€æµ‹ SadTalker ç›®å½•ä¸‹çš„è™šæ‹Ÿç¯å¢ƒ
        sadtalker_venv_python = os.path.join(SADTALKER_DIR, ".venv", "Scripts", "python.exe")

        if os.path.exists(sadtalker_venv_python):
            python_exec = sadtalker_venv_python
            logger.info(f"ä½¿ç”¨ SadTalker è™šæ‹Ÿç¯å¢ƒ: {sadtalker_venv_python}")
        else:
            python_exec = "python"
            logger.warning("æœªæ£€æµ‹åˆ° SadTalker/.venvï¼Œä½¿ç”¨é»˜è®¤ Python")

        # æ„å»º SadTalker å‘½ä»¤
        cmd = [
            python_exec, "inference.py",
            "--driven_audio", audio_path,
            "--source_image", image_path,
            "--result_dir", SADTALKER_OUTPUT_DIR,
            "--still",
            "--preprocess", "crop",
            # "--enhancer", "gfpgan",
            "--batch_size", "4"
        ]

        # åœ¨ SadTalker ç›®å½•ä¸‹æ‰§è¡Œ
        process = subprocess.Popen(
            cmd,
            cwd=SADTALKER_DIR,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        # è¯»å–è¾“å‡º
        output_lines = []
        for line in process.stdout:
            output_lines.append(line)
            logger.info(f"SadTalker: {line.strip()}")

        process.wait()

        if process.returncode != 0:
            logger.error(f"SadTalker æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
            return {"success": False, "error": "è§†é¢‘ç”Ÿæˆå¤±è´¥"}

        # æŸ¥æ‰¾ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆæœ€æ–°çš„ mp4 æ–‡ä»¶ï¼‰
        video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", "*.mp4")
        video_files = glob.glob(video_pattern, recursive=True)

        if not video_files:
            logger.error("æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶")
            return {"success": False, "error": "æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘"}

        # è·å–æœ€æ–°çš„è§†é¢‘æ–‡ä»¶
        latest_video = max(video_files, key=os.path.getmtime)
        video_filename = os.path.basename(latest_video)

        logger.info(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {latest_video}")
        return {
            "success": True,
            "video_path": latest_video,
            "video_filename": video_filename
        }

    except Exception as e:
        logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# ==================== åˆå§‹åŒ–ä»£ç†å®ä¾‹ ====================
agent = PsychologicalAgent(DEEPSEEK_API_KEY)


# ==================== API è·¯ç”± ====================

@app.route('/')
def root_redirect():
    """
    æ ¹è·¯å¾„ - é‡å®šå‘åˆ°é€‰æ‹©é¡µé¢
    """
    return redirect('/select')

@app.route('/index')
def main_page():
    """
    ä¸»é¡µé¢è·¯ç”± - æä¾›ä¸»é¡µé¢
    """
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return """
        <!DOCTYPE html>
        <html>
        <head><title>å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†</title></head>
        <body style="font-family: Arial; padding: 40px; text-align: center;">
            <h1 style="color: #4a90e2;">å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†</h1>
            <p>è¯·å…ˆ <a href="/select">é€‰æ‹©æ•°å­—äººå½¢è±¡</a></p>
        </body>
        </html>
        """


@app.route('/select')
def select_page():
    """é€‰æ‹©æ•°å­—äººå…¥å£é¡µ"""
    try:
        with open('select.html', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return """
        <!DOCTYPE html>
        <html>
        <head><title>é€‰æ‹©æ•°å­—äººå½¢è±¡</title></head>
        <body style="font-family: Arial; padding: 40px; text-align: center;">
            <h1 style="color: #4a90e2;">é€‰æ‹©æ•°å­—äººå½¢è±¡</h1>
            <p>select.html æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚</p>
        </body>
        </html>
        """, 404


@app.route('/avatars/<filename>')
def serve_avatar(filename):
    """
    æä¾›æ•°å­—äººå›¾ç‰‡æœåŠ¡
    """
    try:
        return send_from_directory('avatars', filename)
    except FileNotFoundError:
        logger.error(f"æ•°å­—äººå›¾ç‰‡æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å›¾ç‰‡ {filename} æœªæ‰¾åˆ°"}), 404


@app.route('/uploads/<filename>')
def serve_uploaded_avatar(filename):
    """
    æä¾›ç”¨æˆ·ä¸Šä¼ çš„æ•°å­—äººå›¾ç‰‡æœåŠ¡
    """
    try:
        return send_from_directory('uploads', filename)
    except FileNotFoundError:
        logger.error(f"ä¸Šä¼ çš„å›¾ç‰‡æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å›¾ç‰‡ {filename} æœªæ‰¾åˆ°"}), 404


@app.route('/api/set_avatar', methods=['POST'])
def set_sadtalker_image():
    """
    è®¾ç½® SadTalker ä½¿ç”¨çš„æ•°å­—äººå›¾ç‰‡
    """
    global current_avatar_image

    try:
        data = request.get_json()
        avatar_id = data.get('avatar_id', '1')

        if avatar_id.startswith('upload_'):
            # ä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡
            uploaded_filename = avatar_id.replace('upload_', '')
            avatar_path = os.path.join(UPLOADS_DIR, uploaded_filename)

            if os.path.exists(avatar_path):
                # å°†ä¸Šä¼ çš„å›¾ç‰‡å¤åˆ¶åˆ° SadTalker ç›®å½•
                sadtalker_dest = os.path.join(SADTALKER_DIR, "my_photo.png")
                try:
                    shutil.copy2(avatar_path, sadtalker_dest)
                    current_avatar_image = sadtalker_dest
                    logger.info(f"å·²è®¾ç½®ä¸Šä¼ çš„å›¾ç‰‡ä¸ºæ•°å­—äººå½¢è±¡: {uploaded_filename}")

                    return jsonify({
                        "success": True,
                        "message": f"å·²ä½¿ç”¨æ‚¨ä¸Šä¼ çš„å›¾ç‰‡",
                        "image_url": f"/uploads/{uploaded_filename}"
                    })
                except Exception as e:
                    logger.warning(f"å¤åˆ¶ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
                    return jsonify({"success": False, "error": "è®¾ç½®ä¸Šä¼ å›¾ç‰‡å¤±è´¥"}), 500
            else:
                logger.error(f"ä¸Šä¼ çš„å›¾ç‰‡ä¸å­˜åœ¨: {avatar_path}")
                return jsonify({"success": False, "error": "ä¸Šä¼ çš„å›¾ç‰‡ä¸å­˜åœ¨"}), 404
        else:
            # ä½¿ç”¨é¢„ç½®çš„æ•°å­—äººå›¾ç‰‡
            avatar_images = {
                '1': 'avatar1.png',
                '2': 'avatar2.png',
                '3': 'avatar3.png'
            }

            avatar_filename = avatar_images.get(avatar_id, 'avatar1.png')
            new_image_path = os.path.join('avatars', avatar_filename)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(new_image_path):
                # å°†é¢„ç½®å›¾ç‰‡å¤åˆ¶åˆ° SadTalker ç›®å½•
                sadtalker_dest = os.path.join(SADTALKER_DIR, "my_photo.png")
                try:
                    shutil.copy2(new_image_path, sadtalker_dest)
                    current_avatar_image = sadtalker_dest
                    logger.info(f"å·²æ›´æ–° SadTalker å›¾ç‰‡ä¸º: {new_image_path}")

                    return jsonify({
                        "success": True,
                        "message": f"å·²åˆ‡æ¢ä¸ºé¢„ç½®å½¢è±¡ {avatar_id}",
                        "image_url": f"/avatars/{avatar_filename}"
                    })
                except Exception as e:
                    logger.warning(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {str(e)}")
                    return jsonify({"success": False, "error": "è®¾ç½®å›¾ç‰‡å¤±è´¥"}), 500
            else:
                logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {new_image_path}")
                return jsonify({"success": False, "error": "å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨"}), 404

    except Exception as e:
        logger.error(f"è®¾ç½®æ•°å­—äººå›¾ç‰‡å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/upload_avatar', methods=['POST'])
def upload_avatar():
    """
    ä¸Šä¼ ç”¨æˆ·è‡ªå®šä¹‰æ•°å­—äººå›¾ç‰‡
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'avatar' not in request.files:
            return jsonify({"success": False, "error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400

        file = request.files['avatar']

        # æ£€æŸ¥æ–‡ä»¶å
        if file.filename == '':
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if not allowed_file(file.filename):
            return jsonify({"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚è¯·ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶ (png, jpg, jpeg, gif, bmp, webp)"}), 400

        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        filename = secure_filename(file.filename)
        # æ·»åŠ æ—¶é—´æˆ³å’Œéšæœºå­—ç¬¦ä¸²é¿å…é‡å
        name, ext = os.path.splitext(filename)
        unique_filename = f"{name}_{uuid.uuid4().hex[:8]}{ext}"
        file_path = os.path.join(UPLOADS_DIR, unique_filename)

        # ä¿å­˜åŸå§‹æ–‡ä»¶
        file.save(file_path)
        logger.info(f"ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ä¿å­˜åˆ°: {file_path}")

        # å¤„ç†å›¾ç‰‡ï¼ˆè°ƒæ•´å°ºå¯¸ç­‰ï¼‰
        processed_filename = f"processed_{unique_filename}"
        processed_path = os.path.join(UPLOADS_DIR, processed_filename)

        if process_uploaded_image(file_path, processed_path):
            # å¤„ç†æˆåŠŸï¼Œä½¿ç”¨å¤„ç†åçš„æ–‡ä»¶
            final_path = processed_path
            final_filename = processed_filename
            # åˆ é™¤åŸå§‹æ–‡ä»¶
            try:
                os.remove(file_path)
            except:
                pass
        else:
            # å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶
            final_path = file_path
            final_filename = unique_filename
            logger.warning(f"å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶: {final_path}")

        # ç”Ÿæˆä¸Šä¼ æˆåŠŸçš„å“åº”
        response_data = {
            "success": True,
            "message": "å›¾ç‰‡ä¸Šä¼ æˆåŠŸ",
            "filename": final_filename,
            "image_url": f"/uploads/{final_filename}",
            "avatar_id": f"upload_{final_filename}"
        }

        logger.info(f"ç”¨æˆ·å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {final_filename}")
        return jsonify(response_data)

    except Exception as e:
        logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}), 500


@app.route('/api/get_avatars', methods=['GET'])
def get_available_avatars():
    """
    è·å–å¯ç”¨çš„æ•°å­—äººå½¢è±¡åˆ—è¡¨
    """
    try:
        avatars = []

        # æ·»åŠ é¢„ç½®å½¢è±¡
        for i in range(1, 4):
            avatar_file = f"avatar{i}.png"
            avatar_path = os.path.join(AVATARS_DIR, avatar_file)
            if os.path.exists(avatar_path):
                avatars.append({
                    "id": str(i),
                    "name": f"é¢„ç½®å½¢è±¡ {i}",
                    "type": "preset",
                    "image_url": f"/avatars/{avatar_file}"
                })

        # æ·»åŠ ä¸Šä¼ çš„å½¢è±¡
        upload_files = os.listdir(UPLOADS_DIR)
        for filename in upload_files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰processed_å‰ç¼€ï¼‰
                if not filename.startswith('processed_'):
                    avatars.append({
                        "id": f"upload_{filename}",
                        "name": f"æˆ‘çš„å½¢è±¡: {filename[:20]}...",
                        "type": "uploaded",
                        "image_url": f"/uploads/{filename}"
                    })

        return jsonify({
            "success": True,
            "avatars": avatars,
            "total": len(avatars)
        })

    except Exception as e:
        logger.error(f"è·å–å½¢è±¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


# ==================== è¯­éŸ³è¯†åˆ«API ====================
@app.route('/api/recognize_speech', methods=['POST'])
def recognize_speech():
    """
    è¯­éŸ³è¯†åˆ«æ¥å£

    æ”¯æŒä¸Šä¼ éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    try:
        logger.info("æ”¶åˆ°è¯­éŸ³è¯†åˆ«è¯·æ±‚")

        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†è¯­éŸ³è¯†åˆ«åº“
        if not SPEECH_RECOGNITION_AVAILABLE:
            return jsonify({
                "success": False,
                "error": "è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·å®‰è£…ä¾èµ–: pip install SpeechRecognition pydub"
            }), 501

        # æ£€æŸ¥è¯·æ±‚æ•°æ®
        if 'audio' not in request.files and 'audio_data' not in request.form:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›éŸ³é¢‘æ•°æ®"}), 400

        audio_format = request.form.get('format', 'webm')

        if 'audio' in request.files:
            # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
            audio_file = request.files['audio']
            audio_data = audio_file.read()
        else:
            # å¤„ç†Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
            audio_data_str = request.form.get('audio_data', '')
            if ',' in audio_data_str:
                audio_data_str = audio_data_str.split(',')[1]
            audio_data = base64.b64decode(audio_data_str)

        # è¯†åˆ«è¯­éŸ³
        result = recognize_speech_from_audio(audio_data, audio_format)

        if result["success"]:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            filename = f"speech_{uuid.uuid4().hex[:8]}.{audio_format}"
            save_audio_file(audio_data, filename)

            result["audio_url"] = f"/api/speech/{filename}"
            result["timestamp"] = datetime.datetime.now().isoformat()

            logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {result['text'][:50]}...")
        else:
            logger.warning(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        return jsonify(result)

    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«æ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}",
            "text": ""
        }), 500


@app.route('/api/speech/<filename>')
def serve_speech(filename):
    """
    æä¾›è¯­éŸ³æ–‡ä»¶æœåŠ¡
    """
    try:
        return send_from_directory(SPEECH_INPUT_DIR, filename)
    except FileNotFoundError:
        logger.error(f"è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": "è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """
    å¿ƒç†åˆ†æä¸»æ¥å£ï¼ˆé›†æˆ TTS å’Œæ•°å­—äººè§†é¢‘ç”Ÿæˆï¼‰

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼‰
        - generate_video: æ˜¯å¦ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯é€‰ï¼Œé»˜è®¤ Trueï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«è§†é¢‘ URL
    """
    global current_avatar_image

    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')
        generate_video = data.get('generate_video', True)

        logger.info(f"æ”¶åˆ°åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}, ç”Ÿæˆè§†é¢‘: {generate_video}")

        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # è°ƒç”¨å¿ƒç†åˆ†æä»£ç†
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            result["detected_emotion"] = detected_emotion
            result["timestamp"] = datetime.datetime.now().isoformat()

            # å¦‚æœéœ€è¦ç”Ÿæˆè§†é¢‘
            if generate_video:
                response_text = result.get("response", "")

                # æ¸…ç† HTML æ ‡ç­¾ï¼Œåªä¿ç•™çº¯æ–‡æœ¬ç”¨äº TTS
                import re
                clean_text = re.sub(r'<[^>]+>', '', response_text)
                clean_text = clean_text.replace('<br>', 'ã€‚').replace('&nbsp;', ' ')

                # æ­¥éª¤1: TTS æ–‡å­—è½¬è¯­éŸ³
                logger.info("å¼€å§‹ TTS è½¬æ¢...")
                tts_result = text_to_speech(clean_text)

                if tts_result["success"]:
                    audio_path = tts_result["audio_path"]
                    result["audio_url"] = f"/api/audio/{tts_result['audio_filename']}"

                    # æ­¥éª¤2: SadTalker ç”Ÿæˆè§†é¢‘ï¼ˆä½¿ç”¨å½“å‰è®¾ç½®çš„æ•°å­—äººå›¾ç‰‡ï¼‰
                    logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
                    video_result = generate_talking_video(audio_path, current_avatar_image)

                    if video_result["success"]:
                        result["video_url"] = f"/api/video/{video_result['video_filename']}"
                        result["video_generated"] = True
                        logger.info(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {video_result['video_filename']}")
                    else:
                        result["video_generated"] = False
                        result["video_error"] = video_result.get("error", "è§†é¢‘ç”Ÿæˆå¤±è´¥")
                        logger.warning(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {video_result.get('error')}")
                else:
                    result["video_generated"] = False
                    result["tts_error"] = tts_result.get("error", "TTS è½¬æ¢å¤±è´¥")
                    logger.warning(f"TTS è½¬æ¢å¤±è´¥: {tts_result.get('error')}")

        return jsonify(result)

    except Exception as e:
        logger.error(f"åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/video/<filename>')
def serve_video(filename):
    """
    æä¾›è§†é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: è§†é¢‘æ–‡ä»¶å

    Returns:
        è§†é¢‘æ–‡ä»¶
    """
    # åœ¨ SadTalker è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘
    video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", filename)
    video_files = glob.glob(video_pattern, recursive=True)

    if video_files:
        video_path = video_files[0]
        directory = os.path.dirname(video_path)
        return send_from_directory(directory, filename, mimetype='video/mp4')

    logger.error(f"è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/audio/<filename>')
def serve_audio(filename):
    """
    æä¾›éŸ³é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: éŸ³é¢‘æ–‡ä»¶å

    Returns:
        éŸ³é¢‘æ–‡ä»¶
    """
    audio_path = os.path.join(AUDIO_OUTPUT_DIR, filename)

    if os.path.exists(audio_path):
        return send_from_directory(AUDIO_OUTPUT_DIR, filename, mimetype='audio/mpeg')

    logger.error(f"éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze_local', methods=['POST'])
def analyze_local():
    """
    æœ¬åœ°æ¨¡å‹åˆ†ææ¥å£ï¼ˆå½“å‰ä½¿ç”¨ DeepSeek API ä½œä¸ºåå¤‡ï¼‰

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')

        logger.info(f"æ”¶åˆ°æœ¬åœ°åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}")

        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # å°è¯•ä½¿ç”¨ DeepSeek API
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            result["detected_emotion"] = detected_emotion
            result["timestamp"] = datetime.datetime.now().isoformat()
        else:
            # API å¤±è´¥æ—¶ä½¿ç”¨å¤‡é€‰å›å¤
            result = generate_fallback_response(user_input, detected_emotion)

        return jsonify(result)

    except Exception as e:
        logger.error(f"æœ¬åœ°åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/analyze_emotion', methods=['POST'])
def analyze_emotion():
    """
    è¡¨æƒ…è¯†åˆ«æ¥å£

    è¯·æ±‚ä½“:
        - image: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        JSON æ ¼å¼çš„æƒ…ç»ªåˆ†æç»“æœ
    """
    try:
        data = request.get_json()

        if not data or 'image' not in data:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›å›¾ç‰‡æ•°æ®"}), 400

        logger.info("æ”¶åˆ°è¡¨æƒ…åˆ†æè¯·æ±‚")

        # åˆ†æè¡¨æƒ…
        result = analyze_emotion_from_image(data['image'])

        return jsonify({
            "success": True,
            "dominant_emotion": result["dominant_emotion"],
            "emotion_scores": result["emotion_scores"],
            "face_detected": result["face_detected"],
            "timestamp": datetime.datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"è¡¨æƒ…åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }), 500


@app.route('/api/model/status', methods=['GET'])
def model_status():
    """
    æ¨¡å‹çŠ¶æ€æŸ¥è¯¢æ¥å£

    Returns:
        JSON æ ¼å¼çš„æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    """
    return jsonify({
        "local_model_loaded": False,
        "model_loading": False,
        "deepface_available": DEEPFACE_AVAILABLE,
        "deepseek_api_available": True,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/status', methods=['GET'])
def api_status():
    """
    API çŠ¶æ€æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„ API çŠ¶æ€ä¿¡æ¯
    """
    api_test = test_deepseek_api()

    return jsonify({
        "status": "healthy" if api_test.get("success") else "warning",
        "deepseek_api": api_test,
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„æœåŠ¡å¥åº·çŠ¶æ€
    """
    return jsonify({
        "status": "healthy",
        "service": "å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†",
        "version": "2.3",
        "timestamp": datetime.datetime.now().isoformat(),
        "features": {
            "psychological_analysis": True,
            "emotion_recognition": DEEPFACE_AVAILABLE,
            "real_time_camera": True,
            "deepseek_api": True,
            "avatar_selection": True,
            "avatar_upload": True,
            "speech_input": SPEECH_RECOGNITION_AVAILABLE
        }
    })


@app.route('/api/conversation/summary', methods=['GET'])
def get_conversation_summary():
    """
    è·å–å¯¹è¯æ‘˜è¦

    Returns:
        JSON æ ¼å¼çš„å¯¹è¯æ‘˜è¦ä¿¡æ¯
    """
    user_messages = [msg['content'] for msg in conversation_history if msg['role'] == 'user']

    return jsonify({
        "total_conversations": len(conversation_history) // 2,
        "recent_topics": user_messages[-3:] if user_messages else [],
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/conversation/reset', methods=['POST'])
def reset_conversation():
    """
    é‡ç½®å¯¹è¯å†å²

    Returns:
        JSON æ ¼å¼çš„æ“ä½œç»“æœ
    """
    global conversation_history
    conversation_history = []
    logger.info("å¯¹è¯å†å²å·²é‡ç½®")
    return jsonify({"success": True, "message": "å¯¹è¯å·²é‡ç½®"})


@app.route('/api/debug', methods=['GET'])
def debug_info():
    """
    è°ƒè¯•ä¿¡æ¯æ¥å£ï¼ˆä»…ç”¨äºå¼€å‘ï¼‰

    Returns:
        JSON æ ¼å¼çš„è°ƒè¯•ä¿¡æ¯
    """
    return jsonify({
        "routes": [str(rule) for rule in app.url_map.iter_rules()],
        "conversation_length": len(conversation_history),
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "api_key_set": bool(DEEPSEEK_API_KEY),
        "sadtalker_image": current_avatar_image,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/favicon.ico')
def favicon():
    """å¤„ç† favicon è¯·æ±‚ï¼Œé¿å… 404 é”™è¯¯"""
    return '', 204


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† v2.3")
    print("=" * 60)
    print(f"ğŸ“± æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"ğŸ‘¤ å½¢è±¡é€‰æ‹©: http://localhost:5000/select")
    print(f"ğŸ’¬ ä¸»é¡µé¢: http://localhost:5000/index")
    print(f"ğŸ“¤ æ–°å¢åŠŸèƒ½: ç”¨æˆ·å¯ä¸Šä¼ è‡ªå®šä¹‰æ•°å­—äººå½¢è±¡")
    print(f"ğŸ¤ è¯­éŸ³è¾“å…¥: æ”¯æŒï¼ˆéœ€è¦æµè§ˆå™¨æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼‰")
    print(f"â¤ï¸  å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")
    print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€: http://localhost:5000/api/model/status")
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: http://localhost:5000/api/debug")
    print("=" * 60)
    print("å¯ç”¨ API ç«¯ç‚¹:")
    print("  POST /api/analyze        - å¿ƒç†åˆ†æ")
    print("  POST /api/analyze_local  - æœ¬åœ°æ¨¡å‹åˆ†æ")
    print("  POST /api/analyze_emotion - è¡¨æƒ…è¯†åˆ«")
    print("  POST /api/set_avatar     - è®¾ç½®æ•°å­—äººå½¢è±¡")
    print("  POST /api/upload_avatar  - ä¸Šä¼ è‡ªå®šä¹‰å½¢è±¡")
    print("  GET  /api/get_avatars    - è·å–å¯ç”¨å½¢è±¡åˆ—è¡¨")
    print("  POST /api/recognize_speech - è¯­éŸ³è¯†åˆ«")
    print("  GET  /api/health         - å¥åº·æ£€æŸ¥")
    print("  GET  /api/model/status   - æ¨¡å‹çŠ¶æ€")
    print("  GET  /api/conversation/summary - å¯¹è¯æ‘˜è¦")
    print("  POST /api/conversation/reset   - é‡ç½®å¯¹è¯")
    print("=" * 60)
    
    # æ£€æŸ¥è¯­éŸ³è¯†åˆ«åŠŸèƒ½
    if SPEECH_RECOGNITION_AVAILABLE:
        print("âœ… è¯­éŸ³è¯†åˆ«åŠŸèƒ½å·²å¯ç”¨")
    else:
        print("âš ï¸  è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install SpeechRecognition pydub")
    
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)

    # å¯åŠ¨ Flask æœåŠ¡
    app.run(
        debug=True,
        host='0.0.0.0',
        port=5000,
        use_reloader=False  # ç¦ç”¨è‡ªåŠ¨é‡è½½ï¼Œé¿å…æ¨¡å‹é‡å¤åŠ è½½
    )