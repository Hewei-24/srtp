"""
å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - ä¸»æœåŠ¡å™¨ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰
==============================================

æœ¬æ¨¡å—æä¾›åŸºäº Flask çš„ Web æœåŠ¡ï¼Œé›†æˆä»¥ä¸‹åŠŸèƒ½ï¼š
1. æœ¬åœ°å¿ƒç†å¤§æ¨¡å‹å¿ƒç†å’¨è¯¢æœåŠ¡
2. DeepFace é¢éƒ¨è¡¨æƒ…è¯†åˆ«
3. å¯¹è¯å†å²ç®¡ç†
4. RESTful API æ¥å£
5. æ•°å­—äººå½¢è±¡é€‰æ‹©åŠŸèƒ½
6. è¯­éŸ³è¾“å…¥è¯†åˆ«åŠŸèƒ½
7. ç”¨æˆ·ä¸Šä¼ è‡ªå®šä¹‰æ•°å­—äººå½¢è±¡åŠŸèƒ½

ä½œè€…: SRTP é¡¹ç›®ç»„
ç‰ˆæœ¬: 2.4
"""

import os
import base64
import logging
import datetime
import subprocess
import uuid
import glob
import io
import time 
import wave
import tempfile
import shutil
import threading
import json
import asyncio
from typing import Dict, Any, Optional

import cv2
import numpy as np
import requests
from flask import Flask, request, jsonify, send_from_directory, redirect, url_for, render_template
from flask_cors import CORS
from werkzeug.utils import secure_filename

# ==================== æœ¬åœ°æ¨¡å‹ç›¸å…³å¯¼å…¥ ====================
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    TORCH_AVAILABLE = True
    print("âœ… PyTorch å’Œ Transformers åº“åŠ è½½æˆåŠŸ")
except ImportError as e:
    TORCH_AVAILABLE = False
    print(f"âŒ PyTorch ç›¸å…³åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install torch transformers peft")

# è¯­éŸ³è¯†åˆ«ç›¸å…³å¯¼å…¥
try:
    import speech_recognition as sr
    from pydub import AudioSegment
    SPEECH_RECOGNITION_AVAILABLE = True
    print("âœ… è¯­éŸ³è¯†åˆ«åº“åŠ è½½æˆåŠŸ")
except ImportError as e:
    SPEECH_RECOGNITION_AVAILABLE = False
    print(f"âŒ è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install SpeechRecognition pydub")

# Edge TTS å¯¼å…¥
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
    print("âœ… Edge TTS åº“åŠ è½½æˆåŠŸ")
except ImportError as e:
    EDGE_TTS_AVAILABLE = False
    print(f"âŒ Edge TTS åº“æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install edge-tts")

# ==================== æ—¥å¿—é…ç½® ====================
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== DeepFace å¯¼å…¥ ====================
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    logger.info("DeepFace åº“åŠ è½½æˆåŠŸ")
except ImportError as e:
    DEEPFACE_AVAILABLE = False
    logger.warning(f"DeepFace åº“æœªå®‰è£…: {e}")
    logger.warning("è¯·è¿è¡Œ: pip install deepface")

# ==================== Flask åº”ç”¨åˆå§‹åŒ– ====================
app = Flask(__name__, template_folder='templates', static_folder='static')
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# ==================== é…ç½®å¸¸é‡ ====================
# æœ¬åœ°æ¨¡å‹é…ç½® - ä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„
LOCAL_MODEL_PATH = "models/Qwen1.5-0.5B"
LOCAL_ADAPTER_PATH = "outputs/psychology_trained_model"
MODEL_LOADED = False

# SadTalker é…ç½®
SADTALKER_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "SadTalker")
SADTALKER_IMAGE = os.path.join(SADTALKER_DIR, "my_photo.png")  # æ•°å­—äººå›¾ç‰‡ï¼ˆé»˜è®¤ï¼‰
SADTALKER_OUTPUT_DIR = os.path.join(SADTALKER_DIR, "results")
AUDIO_OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "audio_output")
AVATARS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "avatars")
SPEECH_INPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "speech_input")
UPLOADS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploads")  # æ–°å¢ï¼šç”¨æˆ·ä¸Šä¼ ç›®å½•

# å¾…æœºè§†é¢‘é…ç½®
IDLE_VIDEOS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "idle_videos")
SPEAKING_VIDEOS_DIR = os.path.join(app.static_folder, "speaking_videos")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)
os.makedirs(SADTALKER_OUTPUT_DIR, exist_ok=True)
os.makedirs(AVATARS_DIR, exist_ok=True)
os.makedirs(SPEECH_INPUT_DIR, exist_ok=True)
os.makedirs(UPLOADS_DIR, exist_ok=True)  # æ–°å¢ï¼šåˆ›å»ºä¸Šä¼ ç›®å½•

# å…è®¸ä¸Šä¼ çš„å›¾ç‰‡æ ¼å¼
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}

# æ£€æŸ¥ avatars ç›®å½•ä¸‹æ˜¯å¦æœ‰é»˜è®¤å›¾ç‰‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
default_avatar_path = os.path.join(AVATARS_DIR, "avatar1.png")
if not os.path.exists(default_avatar_path):
    # å°† SadTalker çš„é»˜è®¤å›¾ç‰‡å¤åˆ¶åˆ° avatars ç›®å½•ä½œä¸º avatar1
    if os.path.exists(SADTALKER_IMAGE):
        shutil.copy2(SADTALKER_IMAGE, default_avatar_path)
        logger.info(f"å·²å°†é»˜è®¤æ•°å­—äººå›¾ç‰‡å¤åˆ¶åˆ°: {default_avatar_path}")
    else:
        # åˆ›å»ºä¸‰ä¸ªç¤ºä¾‹å›¾ç‰‡è·¯å¾„
        for i in range(1, 4):
            avatar_path = os.path.join(AVATARS_DIR, f"avatar{i}.png")
            if not os.path.exists(avatar_path):
                logger.warning(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {avatar_path}ï¼Œè¯·æ”¾ç½®ç›¸åº”å›¾ç‰‡æ–‡ä»¶")

# æƒ…ç»ªæ˜ å°„è¡¨
EMOTION_MAP = {
    'angry':    {'icon': 'ğŸ˜ ', 'name': 'ç”Ÿæ°”', 'context': 'æœ‰äº›ç”Ÿæ°”'},
    'disgust':  {'icon': 'ğŸ¤¢', 'name': 'åŒæ¶', 'context': 'æœ‰äº›åæ„Ÿ'},
    'fear':     {'icon': 'ğŸ˜¨', 'name': 'ææƒ§', 'context': 'æ„Ÿåˆ°ç´§å¼ '},
    'happy':    {'icon': 'ğŸ˜Š', 'name': 'å¼€å¿ƒ', 'context': 'çœ‹èµ·æ¥å¿ƒæƒ…ä¸é”™'},
    'sad':      {'icon': 'ğŸ˜¢', 'name': 'æ‚²ä¼¤', 'context': 'æƒ…ç»ªæœ‰äº›ä½è½'},
    'surprise': {'icon': 'ğŸ˜²', 'name': 'æƒŠè®¶', 'context': 'æœ‰äº›æƒŠè®¶'},
    'neutral':  {'icon': 'ğŸ˜', 'name': 'å¹³é™', 'context': 'æƒ…ç»ªå¹³ç¨³'}
}

# é»˜è®¤æƒ…ç»ªåˆ†æ•°ï¼ˆå½“æ£€æµ‹å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
DEFAULT_EMOTION_SCORES = {
    'angry': 0.0, 'disgust': 0.0, 'fear': 0.0,
    'happy': 0.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 100.0
}

# å¯¹è¯å†å²ï¼ˆå…¨å±€å˜é‡ï¼‰
conversation_history: list = []

# å½“å‰ä½¿ç”¨çš„æ•°å­—äººå›¾ç‰‡ï¼ˆå…¨å±€å˜é‡ï¼Œé»˜è®¤ä¸º SadTalker é»˜è®¤å›¾ç‰‡ï¼‰
current_avatar_image = SADTALKER_IMAGE

# æœ¬åœ°æ¨¡å‹å…¨å±€å˜é‡
local_tokenizer = None
local_model = None

# ==================== æœ¬åœ°æ¨¡å‹åŠ è½½å‡½æ•° ====================
def load_local_model():
    """åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æœ¬åœ°æ¨¡å‹"""
    global local_tokenizer, local_model, MODEL_LOADED

    if not TORCH_AVAILABLE:
        logger.error("PyTorch æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹")
        return

    try:
        logger.info("å¼€å§‹åŠ è½½æœ¬åœ°å¿ƒç†å¤§æ¨¡å‹...")
        logger.info(f"æ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
        logger.info(f"é€‚é…å™¨è·¯å¾„: {LOCAL_ADAPTER_PATH}")

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(LOCAL_MODEL_PATH):
            logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}")
            return

        # åŠ è½½åˆ†è¯å™¨
        local_tokenizer = AutoTokenizer.from_pretrained(
            LOCAL_MODEL_PATH,
            trust_remote_code=True
        )
        logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            LOCAL_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

        # åŠ è½½ PEFT é€‚é…å™¨ï¼ˆå¿ƒç†é¢†åŸŸå¾®è°ƒï¼‰
        if os.path.exists(LOCAL_ADAPTER_PATH):
            local_model = PeftModel.from_pretrained(base_model, LOCAL_ADAPTER_PATH)
            logger.info("PEFT é€‚é…å™¨åŠ è½½å®Œæˆ")
        else:
            local_model = base_model
            logger.warning(f"é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {LOCAL_ADAPTER_PATH}")

        local_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        MODEL_LOADED = True
        logger.info("âœ… æœ¬åœ°å¿ƒç†å¤§æ¨¡å‹åŠ è½½å®Œæˆï¼")

    except FileNotFoundError as e:
        logger.error(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        MODEL_LOADED = False
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        MODEL_LOADED = False

# ==================== è¾…åŠ©å‡½æ•° ====================
def allowed_file(filename: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def process_uploaded_image(file_path: str, target_path: str) -> bool:
    """
    å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡ï¼Œç¡®ä¿é€‚åˆ SadTalker ä½¿ç”¨

    Args:
        file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
        target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„

    Returns:
        å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(file_path)
        if img is None:
            logger.error(f"æ— æ³•è¯»å–å›¾ç‰‡: {file_path}")
            return False

        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼Œå¦‚æœå¤ªå¤§åˆ™è°ƒæ•´
        height, width = img.shape[:2]
        max_size = 1024

        if height > max_size or width > max_size:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale = max_size / max(height, width)
            new_height = int(height * scale)
            new_width = int(width * scale)

            # è°ƒæ•´å°ºå¯¸
            img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)
            logger.info(f"è°ƒæ•´å›¾ç‰‡å°ºå¯¸: {width}x{height} -> {new_width}x{new_height}")

        # ä¿å­˜ä¸ºPNGæ ¼å¼ï¼ˆSadTalker æ¨èæ ¼å¼ï¼‰
        cv2.imwrite(target_path, img)
        logger.info(f"å›¾ç‰‡å·²ä¿å­˜ä¸º: {target_path}")

        return True

    except Exception as e:
        logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {str(e)}")
        return False

# ==================== è¯­éŸ³è¯†åˆ«æ¨¡å— ====================
def recognize_speech_from_audio(audio_data: bytes, audio_format: str = "webm") -> Dict[str, Any]:
    """
    ä»éŸ³é¢‘æ•°æ®ä¸­è¯†åˆ«è¯­éŸ³

    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        audio_format: éŸ³é¢‘æ ¼å¼ï¼ˆwebm, wav, mp3ç­‰ï¼‰

    Returns:
        åŒ…å«è¯†åˆ«ç»“æœçš„å­—å…¸
    """
    if not SPEECH_RECOGNITION_AVAILABLE:
        return {
            "success": False,
            "error": "è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…",
            "text": ""
        }

    try:
        recognizer = sr.Recognizer()

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
        with tempfile.NamedTemporaryFile(suffix=f".{audio_format}", delete=False) as tmp_file:
            tmp_file.write(audio_data)
            tmp_file_path = tmp_file.name

        try:
            # ä½¿ç”¨ pydub åŠ è½½éŸ³é¢‘æ–‡ä»¶
            if audio_format == "webm":
                audio = AudioSegment.from_file(tmp_file_path, format="webm")
            elif audio_format == "mp3":
                audio = AudioSegment.from_mp3(tmp_file_path)
            elif audio_format == "wav":
                audio = AudioSegment.from_wav(tmp_file_path)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
                audio = AudioSegment.from_file(tmp_file_path)

            # è½¬æ¢ä¸º wav æ ¼å¼ï¼ˆSpeechRecognition éœ€è¦ï¼‰
            wav_data = io.BytesIO()
            audio.export(wav_data, format="wav")
            wav_data.seek(0)

            # ä½¿ç”¨ SpeechRecognition è¯†åˆ«
            with sr.AudioFile(wav_data) as source:
                # è°ƒæ•´ç¯å¢ƒå™ªå£°
                recognizer.adjust_for_ambient_noise(source, duration=0.5)
                audio_data = recognizer.record(source)

                # è¯†åˆ«è¯­éŸ³
                text = recognizer.recognize_google(audio_data, language="zh-CN")

                logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {text}")

                return {
                    "success": True,
                    "text": text,
                    "confidence": 0.9  # æš‚æ—¶ä½¿ç”¨å›ºå®šå€¼
                }

        except sr.UnknownValueError:
            return {
                "success": False,
                "error": "æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹",
                "text": ""
            }
        except sr.RequestError as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}",
                "text": ""
            }
        except Exception as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«å¤„ç†é”™è¯¯: {e}")
            return {
                "success": False,
                "error": f"å¤„ç†é”™è¯¯: {str(e)}",
                "text": ""
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_file_path)
            except:
                pass

    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "text": ""
        }


def save_audio_file(audio_data: bytes, filename: str) -> str:
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°

    Args:
        audio_data: éŸ³é¢‘å­—èŠ‚æ•°æ®
        filename: æ–‡ä»¶å

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    file_path = os.path.join(SPEECH_INPUT_DIR, filename)
    with open(file_path, 'wb') as f:
        f.write(audio_data)
    return file_path


# ==================== æœ¬åœ°æ¨¡å‹å¿ƒç†åˆ†æå‡½æ•° ====================
def generate_local_model_response(user_input: str, emotion: str = "neutral") -> str:
    """
    ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¿ƒç†å’¨è¯¢å“åº”

    Args:
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

    Returns:
        ç”Ÿæˆçš„å¿ƒç†å’¨è¯¢å“åº”æ–‡æœ¬
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if not MODEL_LOADED or local_model is None or local_tokenizer is None:
        return "æœ¬åœ°æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™..."

    try:
        # è·å–æƒ…ç»ªä¸Šä¸‹æ–‡æè¿°
        emotion_context = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])['context']

        # æ„å»ºæç¤ºè¯
        prompt = f"""ã€å¿ƒç†åŠ©æ‰‹ã€‘æŒ‡ä»¤ï¼šè¯·ä»¥ä¸“ä¸šå¿ƒç†åŠ©æ‰‹çš„èº«ä»½å›åº”ç”¨æˆ·çš„å¿ƒç†é—®é¢˜
è¾“å…¥ï¼šç”¨æˆ·è¯´ï¼š{user_input}
æƒ…ç»ªçŠ¶æ€ï¼š{emotion_context}
å›ç­”ï¼š"""

        # ç¼–ç è¾“å…¥
        inputs = local_tokenizer(prompt, return_tensors="pt").to(local_model.device)

        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = local_model.generate(
                **inputs,
                max_new_tokens=300,      # æœ€å¤§ç”Ÿæˆ token æ•°
                do_sample=True,          # å¯ç”¨é‡‡æ ·
                temperature=0.7,         # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
                top_p=0.9,               # æ ¸é‡‡æ ·å‚æ•°
                repetition_penalty=1.1,  # é‡å¤æƒ©ç½š
                pad_token_id=local_tokenizer.eos_token_id
            )

        # è§£ç è¾“å‡º
        response = local_tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰æç¤ºè¯ï¼‰
        generated_response = response[len(prompt):].strip()

        # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„åç»­å¯¹è¯ï¼‰
        if "ç”¨æˆ·ï¼š" in generated_response:
            generated_response = generated_response.split("ç”¨æˆ·ï¼š")[0].strip()
        if "ã€å¿ƒç†åŠ©æ‰‹ã€‘" in generated_response:
            generated_response = generated_response.split("ã€å¿ƒç†åŠ©æ‰‹ã€‘")[0].strip()

        return generated_response if generated_response else "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œè¯·ç»§ç»­å‘Šè¯‰æˆ‘æ›´å¤šã€‚"

    except Exception as e:
        logger.error(f"æœ¬åœ°æ¨¡å‹ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å†è¯•ä¸€æ¬¡ã€‚"


# ==================== å¿ƒç†åˆ†æä»£ç†ç±»ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ====================
class PsychologicalAgent:
    """
    å¿ƒç†åˆ†æä»£ç†ç±»

    è´Ÿè´£ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œæä¾›å¿ƒç†å’¨è¯¢æœåŠ¡ã€‚
    æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç»“åˆç”¨æˆ·æƒ…ç»ªçŠ¶æ€ç”Ÿæˆä¸ªæ€§åŒ–å›å¤ã€‚
    """

    def __init__(self):
        """
        åˆå§‹åŒ–å¿ƒç†åˆ†æä»£ç†
        """
        self.model_loaded = MODEL_LOADED

    def _build_system_prompt(self, emotion: str) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯

        Args:
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

        Returns:
            åŒ…å«æƒ…ç»ªä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯
        """
        emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
        emotion_context = emotion_info['context']

        return f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¤§å­¦å¿ƒç†å¥åº·é¡¾é—®ï¼Œä¸“é—¨å¸®åŠ©å¤§å­¦ç”Ÿè§£å†³å¿ƒç†é—®é¢˜ã€‚

é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- ç³»ç»Ÿæ£€æµ‹åˆ°ç”¨æˆ·å½“å‰çš„æƒ…ç»ªçŠ¶æ€ä¸ºï¼š{emotion} ({emotion_context})
- è¿™ä¸ªæƒ…ç»ªä¿¡æ¯æ¥è‡ªå®æ—¶é¢éƒ¨è¡¨æƒ…åˆ†æ
- è¯·ç»“åˆç”¨æˆ·æè¿°çš„æ–‡å­—å†…å®¹å’Œæ£€æµ‹åˆ°çš„æƒ…ç»ªçŠ¶æ€ï¼Œæä¾›æ›´ç²¾å‡†çš„å¿ƒç†åˆ†æ

ä½ çš„èŒè´£ï¼š
1. åˆ†æå­¦ç”Ÿçš„å¿ƒç†çŠ¶æ€å’Œæƒ…ç»ªé—®é¢˜
2. æä¾›ä¸“ä¸šã€æ¸©æš–çš„å¿ƒç†æ”¯æŒå’Œå»ºè®®
3. è¯†åˆ«å±æœºæƒ…å†µå¹¶ç»™å‡ºé€‚å½“å»ºè®®
4. ç”¨åŒç†å¿ƒå’Œç†è§£æ¥å›åº”ç”¨æˆ·

è¯·ä»¥æ¸©æš–ã€ä¸“ä¸šã€æ”¯æŒæ€§çš„è¯­æ°”å›åº”ï¼Œé¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€æä¾›å»ºè®®ã€‚"""

    def analyze(self, user_input: str, emotion: str = "neutral") -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå¿ƒç†å’¨è¯¢å›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹ï¼Œé»˜è®¤ä¸º neutral

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ successã€responseã€model_source ç­‰å­—æ®µ
        """
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            if self.model_loaded and MODEL_LOADED:
                logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆ†æ - æƒ…ç»ª: {emotion}, è¾“å…¥: {user_input[:50]}...")

                # æ„å»ºå®Œæ•´çš„æç¤ºè¯
                full_prompt = self._build_system_prompt(emotion) + f"\n\nç”¨æˆ·è¯´: {user_input}\n\nè¯·å›å¤:"

                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå“åº”
                response_text = generate_local_model_response(user_input, emotion)

                if response_text and len(response_text) > 20:  # ç¡®ä¿å“åº”æœ‰æ•ˆ
                    # æ›´æ–°å¯¹è¯å†å²
                    self._update_history(user_input, response_text)

                    return {
                        "success": True,
                        "response": response_text,
                        "model_source": "local_psychology_model"
                    }
                else:
                    # æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰å›å¤
                    logger.warning("æœ¬åœ°æ¨¡å‹ç”Ÿæˆå“åº”æ— æ•ˆï¼Œä½¿ç”¨å¤‡é€‰å›å¤")

            # ä½¿ç”¨å¤‡é€‰å›å¤ç³»ç»Ÿ
            logger.info(f"ä½¿ç”¨å¤‡é€‰å›å¤ç³»ç»Ÿ - æƒ…ç»ª: {emotion}, è¾“å…¥: {user_input[:50]}...")
            return generate_fallback_response(user_input, emotion)

        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": "ç³»ç»Ÿæš‚æ—¶å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            }

    def _update_history(self, user_input: str, assistant_response: str) -> None:
        """
        æ›´æ–°å¯¹è¯å†å²

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            assistant_response: AI å›å¤
        """
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": assistant_response})

        # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘ 10 æ¡ï¼‰
        if len(conversation_history) > 10:
            conversation_history[:] = conversation_history[-10:]


# ==================== è¡¨æƒ…è¯†åˆ«æ¨¡å— ====================
def analyze_emotion_from_image(image_data: str) -> Dict[str, Any]:
    """
    ä» Base64 ç¼–ç çš„å›¾åƒä¸­åˆ†æé¢éƒ¨è¡¨æƒ…

    Args:
        image_data: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        åŒ…å«æƒ…ç»ªåˆ†æç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬ dominant_emotionã€emotion_scoresã€face_detected
    """
    # æ£€æŸ¥ DeepFace æ˜¯å¦å¯ç”¨
    if not DEEPFACE_AVAILABLE:
        logger.warning("DeepFace åº“ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤æƒ…ç»ª")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }

    logger.info("å¼€å§‹ DeepFace è¡¨æƒ…åˆ†æ...")

    try:
        # è§£ç  Base64 å›¾åƒ
        if ',' in image_data:
            image_data = image_data.split(',')[1]

        img_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img is None:
            logger.warning("å›¾åƒè§£ç å¤±è´¥")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # ä½¿ç”¨ DeepFace åˆ†æè¡¨æƒ…
        analysis = DeepFace.analyze(
            img,
            actions=['emotion'],
            detector_backend='opencv',  # ä½¿ç”¨ OpenCV æ£€æµ‹å™¨ï¼ˆé€Ÿåº¦å¿«ï¼‰
            enforce_detection=False,    # ä¸å¼ºåˆ¶æ£€æµ‹åˆ°äººè„¸
            silent=True                 # é™é»˜æ¨¡å¼
        )

        if not analysis:
            logger.warning("DeepFace è¿”å›ç©ºç»“æœ")
            return {
                "dominant_emotion": "neutral",
                "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
                "face_detected": False
            }

        # æå–ç»“æœ
        dominant_emotion = analysis[0]['dominant_emotion']
        emotion_scores = analysis[0]['emotion']

        # æ£€æŸ¥æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸ï¼ˆé€šè¿‡æ£€æŸ¥ face_confidence æˆ– regionï¼‰
        face_region = analysis[0].get('region', {})
        face_confidence = analysis[0].get('face_confidence', 0)

        # è®°å½•è¯¦ç»†çš„åˆ†æç»“æœç”¨äºè°ƒè¯•
        logger.info(f"è¡¨æƒ…åˆ†æç»“æœ: dominant={dominant_emotion}, scores={emotion_scores}")
        logger.info(f"äººè„¸åŒºåŸŸ: {face_region}, ç½®ä¿¡åº¦: {face_confidence}")

        # è½¬æ¢ä¸º Python float ç±»å‹ï¼ˆé¿å… JSON åºåˆ—åŒ–é—®é¢˜ï¼‰
        converted_scores = {k: float(v) for k, v in emotion_scores.items()}

        # åˆ¤æ–­æ˜¯å¦çœŸæ­£æ£€æµ‹åˆ°äººè„¸
        # å¦‚æœäººè„¸åŒºåŸŸå¤ªå°æˆ–ç½®ä¿¡åº¦å¤ªä½ï¼Œå¯èƒ½æ˜¯è¯¯æ£€
        face_detected = True
        if face_region:
            w = face_region.get('w', 0)
            h = face_region.get('h', 0)
            # å¦‚æœæ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°ï¼ˆå°äº50x50åƒç´ ï¼‰ï¼Œè®¤ä¸ºæ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸
            if w < 50 or h < 50:
                logger.warning(f"æ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸå¤ªå°: {w}x{h}")
                face_detected = False

        return {
            "dominant_emotion": dominant_emotion,
            "emotion_scores": converted_scores,
            "face_detected": face_detected
        }

    except Exception as e:
        logger.warning(f"è¡¨æƒ…åˆ†æå¤±è´¥: {str(e)[:100]}")
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }


def generate_fallback_response(user_input: str, emotion: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¤‡é€‰å›å¤ï¼ˆå½“æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ª

    Returns:
        åŒ…å«å¤‡é€‰å›å¤çš„å­—å…¸
    """
    emotion_info = EMOTION_MAP.get(emotion, EMOTION_MAP['neutral'])
    emotion_prefix = f"{emotion_info['context']}ï¼Œ" if emotion != 'neutral' else ""

    # å…³é”®è¯åŒ¹é…å›å¤
    keyword_responses = {
        'å‹åŠ›': f"{emotion_prefix}å¯¹äºå‹åŠ›é—®é¢˜ï¼Œå»ºè®®ï¼š<br>1. æ·±å‘¼å¸æ”¾æ¾ç»ƒä¹ <br>2. åˆç†å®‰æ’æ—¶é—´å’Œä¼˜å…ˆçº§<br>3. é€‚é‡è¿åŠ¨é‡Šæ”¾å‹åŠ›<br>4. ä¸æœ‹å‹æˆ–å®¶äººå€¾è¯‰",
        'ç„¦è™‘': f"{emotion_prefix}åº”å¯¹ç„¦è™‘çš„æ–¹æ³•ï¼š<br>1. æ­£å¿µå†¥æƒ³ç»ƒä¹ <br>2. å†™ä¸‹æ‹…å¿§äº‹é¡¹<br>3. æ¸è¿›å¼è‚Œè‚‰æ”¾æ¾<br>4. ä¿æŒè§„å¾‹ä½œæ¯",
        'å¤±çœ ': f"{emotion_prefix}æ”¹å–„ç¡çœ çš„å»ºè®®ï¼š<br>1. ç¡å‰1å°æ—¶ä¸ä½¿ç”¨ç”µå­è®¾å¤‡<br>2. åˆ›é€ èˆ’é€‚çš„ç¡çœ ç¯å¢ƒ<br>3. ä¿æŒè§„å¾‹çš„ä½œæ¯æ—¶é—´<br>4. é¿å…ç¡å‰æ‘„å…¥å’–å•¡å› ",
        'æŠ‘éƒ': f"{emotion_prefix}å¦‚æœæŒç»­æƒ…ç»ªä½è½ï¼š<br>1. å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢<br>2. ä¿æŒé€‚åº¦çš„ç¤¾äº¤æ´»åŠ¨<br>3. åšæŒé€‚é‡è¿åŠ¨<br>4. ç»™è‡ªå·±ä¸€äº›æ—¶é—´å’Œè€å¿ƒ",
        'å­¦ä¹ ': f"{emotion_prefix}å­¦ä¹ å‹åŠ›ç®¡ç†ï¼š<br>1. åˆ¶å®šåˆç†çš„å­¦ä¹ è®¡åˆ’<br>2. ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•æé«˜æ•ˆç‡<br>3. ä¿è¯å……è¶³çš„ä¼‘æ¯æ—¶é—´<br>4. ä¸åŒå­¦äº¤æµå­¦ä¹ å¿ƒå¾—"
    }

    # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
    for keyword, response in keyword_responses.items():
        if keyword in user_input.lower():
            return {
                "success": True,
                "response": response,
                "detected_emotion": emotion,
                "model_source": "fallback_system",
                "timestamp": datetime.datetime.now().isoformat()
            }

    # é€šç”¨å›å¤
    return {
        "success": True,
        "response": f"{emotion_prefix}æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ã€‚ä½œä¸ºå¿ƒç†åŠ©æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥æ›´è¯¦ç»†åœ°æè¿°å…·ä½“æƒ…å†µå’Œæ„Ÿå—ï¼Œè¿™æ ·æˆ‘èƒ½æä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„å¸®åŠ©ã€‚",
        "detected_emotion": emotion,
        "model_source": "fallback_system",
        "timestamp": datetime.datetime.now().isoformat()
    }


# ==================== Edge TTS æ–‡å­—è½¬è¯­éŸ³æ¨¡å— ====================
def text_to_speech(text: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ Edge TTS å°†æ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³
    
    æ”¯æŒçš„ä¸­æ–‡éŸ³è‰²ï¼š
    - zh-CN-XiaoxiaoNeural (æ™“æ™“ï¼Œå¥³å£°ï¼Œæ¨è)
    - zh-CN-YunxiNeural (äº‘å¸Œï¼Œç”·å£°)
    - zh-CN-YunxiaNeural (äº‘å¤ï¼Œç”·å£°)
    - zh-CN-YunyangNeural (äº‘æ‰¬ï¼Œç”·å£°)
    """
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        audio_filename = f"tts_{uuid.uuid4().hex[:8]}.mp3"
        audio_path = os.path.join(AUDIO_OUTPUT_DIR, audio_filename)

        logger.info(f"å¼€å§‹ Edge TTS è½¬æ¢ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")
        
        # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ HTML æ ‡ç­¾
        import re
        clean_text = re.sub(r'<[^>]+>', '', text).strip()
        
        if len(clean_text) > 1000:
            clean_text = clean_text[:1000] + "..."
            logger.warning(f"æ–‡æœ¬è¿‡é•¿ï¼Œæˆªæ–­ä¸º1000å­—ç¬¦")
        
        # å¼‚æ­¥ç”Ÿæˆè¯­éŸ³
        async def generate_speech():
            communicate = edge_tts.Communicate(
                clean_text,
                "zh-CN-XiaoxiaoNeural",  # ä¸­æ–‡å¥³å£°éŸ³è‰²
                rate="+0%",              # è¯­é€Ÿ
                volume="+0%"             # éŸ³é‡
            )
            await communicate.save(audio_path)
        
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(generate_speech())
        loop.close()
        
        # éªŒè¯æ–‡ä»¶
        file_size = os.path.getsize(audio_path)
        logger.info(f"éŸ³é¢‘æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {audio_path}, å¤§å°: {file_size} bytes")
        
        if file_size < 100:  # æ–‡ä»¶å¤ªå°å¯èƒ½æ˜¯é”™è¯¯
            logger.error(f"ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½å¤±è´¥: {file_size} bytes")
            return create_silent_audio(audio_filename, audio_path)
        
        return {
            "success": True,
            "audio_path": audio_path,
            "audio_filename": audio_filename,
            "tts_engine": "edge-tts",
            "voice": "zh-CN-XiaoxiaoNeural"
        }

    except Exception as e:
        logger.error(f"Edge TTS è½¬æ¢å¤±è´¥: {str(e)}", exc_info=True)
        return create_silent_audio(audio_filename, audio_path)

def create_silent_audio(filename: str, path: str) -> Dict[str, Any]:
    """
    åˆ›å»ºé™éŸ³éŸ³é¢‘ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    """
    try:
        # ä½¿ç”¨ pydub åˆ›å»ºé™éŸ³éŸ³é¢‘
        silent_audio = AudioSegment.silent(duration=1000)  # 1ç§’é™éŸ³
        silent_audio.export(path, format="mp3")
        
        logger.warning(f"åˆ›å»ºé™éŸ³éŸ³é¢‘: {path}")
        return {
            "success": True,
            "audio_path": path,
            "audio_filename": filename,
            "tts_engine": "silent_audio",
            "is_silent": True,
            "warning": "TTSå¤±è´¥ï¼Œä½¿ç”¨é™éŸ³éŸ³é¢‘"
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºé™éŸ³éŸ³é¢‘å¤±è´¥: {str(e)}")
        return {
            "success": False, 
            "error": f"æ‰€æœ‰TTSæ–¹æ¡ˆéƒ½å¤±è´¥äº†: {str(e)}"
        }

# ==================== SadTalker è§†é¢‘ç”Ÿæˆæ¨¡å— ====================
def generate_talking_video(audio_path: str, image_path: str = None) -> Dict[str, Any]:
    """
    ä½¿ç”¨ SadTalker ç”Ÿæˆæ•°å­—äººè¯´è¯è§†é¢‘

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        image_path: æ•°å­—äººå›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰è®¾ç½®çš„å›¾ç‰‡ï¼‰

    Returns:
        åŒ…å«è§†é¢‘æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    try:
        # ä½¿ç”¨æŒ‡å®šçš„å›¾ç‰‡æˆ–é»˜è®¤å›¾ç‰‡
        if image_path is None:
            image_path = current_avatar_image

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return {"success": False, "error": "æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨"}

        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return {"success": False, "error": "éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}

        logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
        logger.info(f"å›¾ç‰‡: {image_path}")
        logger.info(f"éŸ³é¢‘: {audio_path}")

        # æ£€æµ‹ SadTalker ç›®å½•ä¸‹çš„è™šæ‹Ÿç¯å¢ƒ
        sadtalker_venv_python = os.path.join(SADTALKER_DIR, ".venv", "Scripts", "python.exe")

        if os.path.exists(sadtalker_venv_python):
            python_exec = sadtalker_venv_python
            logger.info(f"ä½¿ç”¨ SadTalker è™šæ‹Ÿç¯å¢ƒ: {sadtalker_venv_python}")
        else:
            python_exec = "python"
            logger.warning("æœªæ£€æµ‹åˆ° SadTalker/.venvï¼Œä½¿ç”¨é»˜è®¤ Python")

        # æ„å»º SadTalker å‘½ä»¤
        cmd = [
            python_exec, "inference.py",
            "--driven_audio", audio_path,
            "--source_image", image_path,
            "--result_dir", SADTALKER_OUTPUT_DIR,
            "--still",
            "--preprocess", "crop",
            # "--enhancer", "gfpgan",
            "--batch_size", "4"
        ]

        # åœ¨ SadTalker ç›®å½•ä¸‹æ‰§è¡Œ
        process = subprocess.Popen(
            cmd,
            cwd=SADTALKER_DIR,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        # è¯»å–è¾“å‡º
        output_lines = []
        for line in process.stdout:
            output_lines.append(line)
            logger.info(f"SadTalker: {line.strip()}")

        process.wait()

        if process.returncode != 0:
            logger.error(f"SadTalker æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
            return {"success": False, "error": "è§†é¢‘ç”Ÿæˆå¤±è´¥"}

        # æŸ¥æ‰¾ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆæœ€æ–°çš„ mp4 æ–‡ä»¶ï¼‰
        video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", "*.mp4")
        video_files = glob.glob(video_pattern, recursive=True)

        if not video_files:
            logger.error("æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶")
            return {"success": False, "error": "æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘"}

        # è·å–æœ€æ–°çš„è§†é¢‘æ–‡ä»¶
        latest_video = max(video_files, key=os.path.getmtime)
        video_filename = os.path.basename(latest_video)

        logger.info(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {latest_video}")
        return {
            "success": True,
            "video_path": latest_video,
            "video_filename": video_filename
        }

    except Exception as e:
        logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# ==================== åˆå§‹åŒ–ä»£ç†å®ä¾‹ ====================
agent = PsychologicalAgent()

# åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æœ¬åœ°æ¨¡å‹
if TORCH_AVAILABLE:
    model_thread = threading.Thread(target=load_local_model, daemon=True)
    model_thread.start()
    logger.info("å·²å¯åŠ¨æœ¬åœ°æ¨¡å‹åŠ è½½çº¿ç¨‹")


# ==================== API è·¯ç”± ====================

@app.route('/')
def root_redirect():
    return redirect('/select')

@app.route('/index')
def main_page():
    # ä½¿ç”¨ render_template
    return render_template('index.html')

@app.route('/select')
def select_page():
    # ä½¿ç”¨ render_template
    return render_template('select.html')


@app.route('/avatars/<filename>')
def serve_avatar(filename):
    """
    æä¾›æ•°å­—äººå›¾ç‰‡æœåŠ¡
    """
    try:
        return send_from_directory('avatars', filename)
    except FileNotFoundError:
        logger.error(f"æ•°å­—äººå›¾ç‰‡æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å›¾ç‰‡ {filename} æœªæ‰¾åˆ°"}), 404


@app.route('/uploads/<filename>')
def serve_uploaded_avatar(filename):
    """
    æä¾›ç”¨æˆ·ä¸Šä¼ çš„æ•°å­—äººå›¾ç‰‡æœåŠ¡
    """
    try:
        return send_from_directory('uploads', filename)
    except FileNotFoundError:
        logger.error(f"ä¸Šä¼ çš„å›¾ç‰‡æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å›¾ç‰‡ {filename} æœªæ‰¾åˆ°"}), 404


@app.route('/api/set_avatar', methods=['POST'])
def set_sadtalker_image():
    """
    è®¾ç½® SadTalker ä½¿ç”¨çš„æ•°å­—äººå›¾ç‰‡
    """
    global current_avatar_image

    try:
        data = request.get_json()
        avatar_id = data.get('avatar_id', '1')

        if avatar_id.startswith('upload_'):
            # ä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡
            uploaded_filename = avatar_id.replace('upload_', '')
            avatar_path = os.path.join(UPLOADS_DIR, uploaded_filename)

            if os.path.exists(avatar_path):
                # å°†ä¸Šä¼ çš„å›¾ç‰‡å¤åˆ¶åˆ° SadTalker ç›®å½•
                sadtalker_dest = os.path.join(SADTALKER_DIR, "my_photo.png")
                try:
                    shutil.copy2(avatar_path, sadtalker_dest)
                    current_avatar_image = sadtalker_dest
                    logger.info(f"å·²è®¾ç½®ä¸Šä¼ çš„å›¾ç‰‡ä¸ºæ•°å­—äººå½¢è±¡: {uploaded_filename}")

                    return jsonify({
                        "success": True,
                        "message": f"å·²ä½¿ç”¨æ‚¨ä¸Šä¼ çš„å›¾ç‰‡",
                        "image_url": f"/uploads/{uploaded_filename}"
                    })
                except Exception as e:
                    logger.warning(f"å¤åˆ¶ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
                    return jsonify({"success": False, "error": "è®¾ç½®ä¸Šä¼ å›¾ç‰‡å¤±è´¥"}), 500
            else:
                logger.error(f"ä¸Šä¼ çš„å›¾ç‰‡ä¸å­˜åœ¨: {avatar_path}")
                return jsonify({"success": False, "error": "ä¸Šä¼ çš„å›¾ç‰‡ä¸å­˜åœ¨"}), 404
        else:
            # ä½¿ç”¨é¢„ç½®çš„æ•°å­—äººå›¾ç‰‡
            avatar_images = {
                '1': 'avatar1.png',
                '2': 'avatar2.png',
                '3': 'avatar3.png'
            }

            avatar_filename = avatar_images.get(avatar_id, 'avatar1.png')
            new_image_path = os.path.join('avatars', avatar_filename)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(new_image_path):
                # å°†é¢„ç½®å›¾ç‰‡å¤åˆ¶åˆ° SadTalker ç›®å½•
                sadtalker_dest = os.path.join(SADTALKER_DIR, "my_photo.png")
                try:
                    shutil.copy2(new_image_path, sadtalker_dest)
                    current_avatar_image = sadtalker_dest
                    logger.info(f"å·²æ›´æ–° SadTalker å›¾ç‰‡ä¸º: {new_image_path}")

                    return jsonify({
                        "success": True,
                        "message": f"å·²åˆ‡æ¢ä¸ºé¢„ç½®å½¢è±¡ {avatar_id}",
                        "image_url": f"/avatars/{avatar_filename}"
                    })
                except Exception as e:
                    logger.warning(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {str(e)}")
                    return jsonify({"success": False, "error": "è®¾ç½®å›¾ç‰‡å¤±è´¥"}), 500
            else:
                logger.error(f"æ•°å­—äººå›¾ç‰‡ä¸å­˜åœ¨: {new_image_path}")
                return jsonify({"success": False, "error": "å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨"}), 404

    except Exception as e:
        logger.error(f"è®¾ç½®æ•°å­—äººå›¾ç‰‡å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/upload_avatar', methods=['POST'])
def upload_avatar():
    """
    ä¸Šä¼ ç”¨æˆ·è‡ªå®šä¹‰æ•°å­—äººå›¾ç‰‡
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'avatar' not in request.files:
            return jsonify({"success": False, "error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400

        file = request.files['avatar']

        # æ£€æŸ¥æ–‡ä»¶å
        if file.filename == '':
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if not allowed_file(file.filename):
            return jsonify({"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚è¯·ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶ (png, jpg, jpeg, gif, bmp, webp)"}), 400

        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        filename = secure_filename(file.filename)
        # æ·»åŠ æ—¶é—´æˆ³å’Œéšæœºå­—ç¬¦ä¸²é¿å…é‡å
        name, ext = os.path.splitext(filename)
        unique_filename = f"{name}_{uuid.uuid4().hex[:8]}{ext}"
        file_path = os.path.join(UPLOADS_DIR, unique_filename)

        # ä¿å­˜åŸå§‹æ–‡ä»¶
        file.save(file_path)
        logger.info(f"ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ä¿å­˜åˆ°: {file_path}")

        # å¤„ç†å›¾ç‰‡ï¼ˆè°ƒæ•´å°ºå¯¸ç­‰ï¼‰
        processed_filename = f"processed_{unique_filename}"
        processed_path = os.path.join(UPLOADS_DIR, processed_filename)

        if process_uploaded_image(file_path, processed_path):
            # å¤„ç†æˆåŠŸï¼Œä½¿ç”¨å¤„ç†åçš„æ–‡ä»¶
            final_path = processed_path
            final_filename = processed_filename
            # åˆ é™¤åŸå§‹æ–‡ä»¶
            try:
                os.remove(file_path)
            except:
                pass
        else:
            # å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶
            final_path = file_path
            final_filename = unique_filename
            logger.warning(f"å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶: {final_path}")

        # ç”Ÿæˆä¸Šä¼ æˆåŠŸçš„å“åº”
        response_data = {
            "success": True,
            "message": "å›¾ç‰‡ä¸Šä¼ æˆåŠŸ",
            "filename": final_filename,
            "image_url": f"/uploads/{final_filename}",
            "avatar_id": f"upload_{final_filename}"
        }

        logger.info(f"ç”¨æˆ·å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {final_filename}")
        return jsonify(response_data)

    except Exception as e:
        logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}), 500


@app.route('/api/get_avatars', methods=['GET'])
def get_available_avatars():
    """
    è·å–å¯ç”¨çš„æ•°å­—äººå½¢è±¡åˆ—è¡¨
    """
    try:
        avatars = []

        # æ·»åŠ é¢„ç½®å½¢è±¡
        for i in range(1, 4):
            avatar_file = f"avatar{i}.png"
            avatar_path = os.path.join(AVATARS_DIR, avatar_file)
            if os.path.exists(avatar_path):
                avatars.append({
                    "id": str(i),
                    "name": f"é¢„ç½®å½¢è±¡ {i}",
                    "type": "preset",
                    "image_url": f"/avatars/{avatar_file}"
                })

        # æ·»åŠ ä¸Šä¼ çš„å½¢è±¡
        upload_files = os.listdir(UPLOADS_DIR)
        for filename in upload_files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰processed_å‰ç¼€ï¼‰
                if not filename.startswith('processed_'):
                    avatars.append({
                        "id": f"upload_{filename}",
                        "name": f"æˆ‘çš„å½¢è±¡: {filename[:20]}...",
                        "type": "uploaded",
                        "image_url": f"/uploads/{filename}"
                    })

        return jsonify({
            "success": True,
            "avatars": avatars,
            "total": len(avatars)
        })

    except Exception as e:
        logger.error(f"è·å–å½¢è±¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


# ==================== è¯­éŸ³è¯†åˆ«API ====================
@app.route('/api/recognize_speech', methods=['POST'])
def recognize_speech():
    """
    è¯­éŸ³è¯†åˆ«æ¥å£

    æ”¯æŒä¸Šä¼ éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    try:
        logger.info("æ”¶åˆ°è¯­éŸ³è¯†åˆ«è¯·æ±‚")

        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†è¯­éŸ³è¯†åˆ«åº“
        if not SPEECH_RECOGNITION_AVAILABLE:
            return jsonify({
                "success": False,
                "error": "è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·å®‰è£…ä¾èµ–: pip install SpeechRecognition pydub"
            }), 501

        # æ£€æŸ¥è¯·æ±‚æ•°æ®
        if 'audio' not in request.files and 'audio_data' not in request.form:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›éŸ³é¢‘æ•°æ®"}), 400

        audio_format = request.form.get('format', 'webm')

        if 'audio' in request.files:
            # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
            audio_file = request.files['audio']
            audio_data = audio_file.read()
        else:
            # å¤„ç†Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
            audio_data_str = request.form.get('audio_data', '')
            if ',' in audio_data_str:
                audio_data_str = audio_data_str.split(',')[1]
            audio_data = base64.b64decode(audio_data_str)

        # è¯†åˆ«è¯­éŸ³
        result = recognize_speech_from_audio(audio_data, audio_format)

        if result["success"]:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            filename = f"speech_{uuid.uuid4().hex[:8]}.{audio_format}"
            save_audio_file(audio_data, filename)

            result["audio_url"] = f"/api/speech/{filename}"
            result["timestamp"] = datetime.datetime.now().isoformat()

            logger.info(f"è¯­éŸ³è¯†åˆ«æˆåŠŸ: {result['text'][:50]}...")
        else:
            logger.warning(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        return jsonify(result)

    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«æ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}",
            "text": ""
        }), 500


@app.route('/api/speech/<filename>')
def serve_speech(filename):
    """
    æä¾›è¯­éŸ³æ–‡ä»¶æœåŠ¡
    """
    try:
        return send_from_directory(SPEECH_INPUT_DIR, filename)
    except FileNotFoundError:
        logger.error(f"è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": "è¯­éŸ³æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """
    ä¿®æ”¹åçš„åˆ†ææ¥å£ï¼šä»…ç”Ÿæˆ TTS éŸ³é¢‘ï¼Œå¹¶è¿”å›é¢„ç½®è§†é¢‘è·¯å¾„
    """
    try:
        data = request.get_json()
        logger.info(f"æ”¶åˆ°åˆ†æè¯·æ±‚: {data}")
        
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')
        avatar_id = data.get('avatar_id', '1')  # ä»è¯·æ±‚ä¸­è·å– avatar_id
        
        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # 1. è°ƒç”¨å¿ƒç†åˆ†æ (LLM)
        logger.info(f"å¼€å§‹å¿ƒç†åˆ†æ - è¾“å…¥: {user_input[:50]}..., æƒ…ç»ª: {detected_emotion}, å¤´åƒ: {avatar_id}")
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            # 2. ç”Ÿæˆ TTS éŸ³é¢‘
            response_text = result.get("response", "")
            logger.info(f"å¿ƒç†åˆ†ææˆåŠŸï¼Œç”Ÿæˆ TTSï¼Œæ–‡æœ¬é•¿åº¦: {len(response_text)}")
            
            # æ¸…ç†æ–‡æœ¬
            import re
            clean_text = re.sub(r'<[^>]+>', '', response_text).replace('&nbsp;', ' ')
            
            tts_result = text_to_speech(clean_text)

            if tts_result["success"]:
                # 3. è¿”å›éŸ³é¢‘ URL å’Œå¯¹åº”çš„é¢„ç½®è¯´è¯è§†é¢‘ URL
                audio_url = f"/api/audio/{tts_result['audio_filename']}"
                
                # 4. æ ¹æ® avatar_id è¿”å›å¯¹åº”çš„é¢„ç½®è§†é¢‘
                video_filename = f"avatar{avatar_id}_talking.mp4"
                video_path = os.path.join(app.static_folder, "speaking_videos", video_filename)
                
                logger.info(f"æ£€æŸ¥è§†é¢‘æ–‡ä»¶: {video_path}")
                
                if os.path.exists(video_path):
                    video_url = f"/static/speaking_videos/{video_filename}"
                    logger.info(f"ä½¿ç”¨é¢„ç½®è§†é¢‘: {video_url}")
                else:
                    # å¦‚æœç‰¹å®š avatar çš„è§†é¢‘ä¸å­˜åœ¨ï¼Œä½¿ç”¨ avatar1 ä½œä¸ºé»˜è®¤
                    video_filename = "avatar1_talking.mp4"
                    video_url = f"/static/speaking_videos/{video_filename}"
                    logger.warning(f"é¢„ç½®è§†é¢‘ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤: {video_filename}")
                
                result["audio_url"] = audio_url
                result["video_url"] = video_url
                result["video_generated"] = True
                result["is_preset"] = True
                result["avatar_id"] = avatar_id
                result["tts_engine"] = tts_result.get("tts_engine", "edge-tts")
                
                logger.info(f"è¿”å›ç»“æœ: audio={audio_url}, video={video_url}")
            else:
                logger.error(f"TTS ç”Ÿæˆå¤±è´¥: {tts_result.get('error')}")
                result["video_generated"] = False
                result["error"] = tts_result.get('error', 'TTS ç”Ÿæˆå¤±è´¥')

        return jsonify(result)

    except Exception as e:
        logger.error(f"åˆ†ææ¥å£é”™è¯¯: {str(e)}", exc_info=True)
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/video/<filename>')
def serve_video(filename):
    """
    æä¾›è§†é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: è§†é¢‘æ–‡ä»¶å

    Returns:
        è§†é¢‘æ–‡ä»¶
    """
    # åœ¨ SadTalker è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘
    video_pattern = os.path.join(SADTALKER_OUTPUT_DIR, "**", filename)
    video_files = glob.glob(video_pattern, recursive=True)

    if video_files:
        video_path = video_files[0]
        directory = os.path.dirname(video_path)
        return send_from_directory(directory, filename, mimetype='video/mp4')

    logger.error(f"è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/audio/<filename>')
def serve_audio(filename):
    """
    æä¾›éŸ³é¢‘æ–‡ä»¶æœåŠ¡

    Args:
        filename: éŸ³é¢‘æ–‡ä»¶å

    Returns:
        éŸ³é¢‘æ–‡ä»¶
    """
    audio_path = os.path.join(AUDIO_OUTPUT_DIR, filename)

    if os.path.exists(audio_path):
        return send_from_directory(AUDIO_OUTPUT_DIR, filename, mimetype='audio/mpeg')

    logger.error(f"éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    return jsonify({"error": "éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"}), 404


@app.route('/api/analyze_local', methods=['POST'])
def analyze_local():
    """
    æœ¬åœ°æ¨¡å‹åˆ†ææ¥å£ï¼ˆä¸ä¸»æ¥å£ç›¸åŒï¼Œä¿æŒå…¼å®¹æ€§ï¼‰

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')

        logger.info(f"æ”¶åˆ°æœ¬åœ°åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}")

        if not user_input:
            return jsonify({"success": False, "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"}), 400

        # è°ƒç”¨å¿ƒç†åˆ†æä»£ç†
        result = agent.analyze(user_input, detected_emotion)

        if result["success"]:
            result["detected_emotion"] = detected_emotion
            result["timestamp"] = datetime.datetime.now().isoformat()

        return jsonify(result)

    except Exception as e:
        logger.error(f"æœ¬åœ°åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500

@app.route('/api/idle_videos/<filename>')
def serve_idle_video(filename):
    """
    æä¾›å¾…æœºè§†é¢‘æ–‡ä»¶æœåŠ¡
    """
    try:
        return send_from_directory(IDLE_VIDEOS_DIR, filename)
    except FileNotFoundError:
        logger.error(f"å¾…æœºè§†é¢‘æœªæ‰¾åˆ°: {filename}")
        return jsonify({"error": f"å¾…æœºè§†é¢‘ {filename} æœªæ‰¾åˆ°"}), 404

@app.route('/api/analyze_emotion', methods=['POST'])
def analyze_emotion():
    """
    è¡¨æƒ…è¯†åˆ«æ¥å£

    è¯·æ±‚ä½“:
        - image: Base64 ç¼–ç çš„å›¾åƒæ•°æ®

    Returns:
        JSON æ ¼å¼çš„æƒ…ç»ªåˆ†æç»“æœ
    """
    try:
        data = request.get_json()

        if not data or 'image' not in data:
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›å›¾ç‰‡æ•°æ®"}), 400

        logger.info("æ”¶åˆ°è¡¨æƒ…åˆ†æè¯·æ±‚")

        # åˆ†æè¡¨æƒ…
        result = analyze_emotion_from_image(data['image'])

        return jsonify({
            "success": True,
            "dominant_emotion": result["dominant_emotion"],
            "emotion_scores": result["emotion_scores"],
            "face_detected": result["face_detected"],
            "timestamp": datetime.datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"è¡¨æƒ…åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "dominant_emotion": "neutral",
            "emotion_scores": DEFAULT_EMOTION_SCORES.copy(),
            "face_detected": False
        }), 500


@app.route('/api/model/status', methods=['GET'])
def model_status():
    """
    æ¨¡å‹çŠ¶æ€æŸ¥è¯¢æ¥å£

    Returns:
        JSON æ ¼å¼çš„æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    """
    return jsonify({
        "local_model_loaded": MODEL_LOADED,
        "torch_available": TORCH_AVAILABLE,
        "model_loading": TORCH_AVAILABLE and not MODEL_LOADED,
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "edge_tts_available": EDGE_TTS_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/status', methods=['GET'])
def api_status():
    """
    API çŠ¶æ€æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„ API çŠ¶æ€ä¿¡æ¯
    """
    model_status_info = {
        "local_model_loaded": MODEL_LOADED,
        "torch_available": TORCH_AVAILABLE,
        "model_loading": TORCH_AVAILABLE and not MODEL_LOADED
    }

    return jsonify({
        "status": "healthy" if (MODEL_LOADED or not TORCH_AVAILABLE) else "loading",
        "model_status": model_status_info,
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "edge_tts_available": EDGE_TTS_AVAILABLE,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„æœåŠ¡å¥åº·çŠ¶æ€
    """
    return jsonify({
        "status": "healthy",
        "service": "å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç†ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰",
        "version": "2.4",
        "timestamp": datetime.datetime.now().isoformat(),
        "features": {
            "local_psychological_analysis": MODEL_LOADED,
            "fallback_system": True,
            "emotion_recognition": DEEPFACE_AVAILABLE,
            "real_time_camera": True,
            "avatar_selection": True,
            "avatar_upload": True,
            "speech_input": SPEECH_RECOGNITION_AVAILABLE,
            "edge_tts": EDGE_TTS_AVAILABLE,  # æ–°å¢ Edge TTS åŠŸèƒ½
            "idle_animation": True
        }
    })


@app.route('/api/conversation/summary', methods=['GET'])
def get_conversation_summary():
    """
    è·å–å¯¹è¯æ‘˜è¦

    Returns:
        JSON æ ¼å¼çš„å¯¹è¯æ‘˜è¦ä¿¡æ¯
    """
    user_messages = [msg['content'] for msg in conversation_history if msg['role'] == 'user']

    return jsonify({
        "total_conversations": len(conversation_history) // 2,
        "recent_topics": user_messages[-3:] if user_messages else [],
        "timestamp": datetime.datetime.now().isoformat()
    })


def get_idle_videos() -> list:
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„å¾…æœºè§†é¢‘
    """
    try:
        videos = []

        # è·å–ç›®å½•ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶
        video_extensions = ('.mp4', '.webm', '.mov', '.avi')

        for filename in os.listdir(IDLE_VIDEOS_DIR):
            if filename.lower().endswith(video_extensions):
                videos.append({
                    "filename": filename,
                    "url": f"/api/idle_videos/{filename}",  # ä¿®æ”¹ä¸ºæ­£ç¡®çš„ API URL
                    "size": os.path.getsize(os.path.join(IDLE_VIDEOS_DIR, filename)),
                    "modified": os.path.getmtime(os.path.join(IDLE_VIDEOS_DIR, filename))
                })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        videos.sort(key=lambda x: x["modified"], reverse=True)

        logger.info(f"æ‰¾åˆ° {len(videos)} ä¸ªå¾…æœºè§†é¢‘")
        return videos

    except Exception as e:
        logger.error(f"è·å–å¾…æœºè§†é¢‘åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []

def get_idle_video_for_avatar(avatar_id: str) -> Optional[str]:
    try:
        # å…ˆå°è¯•æŸ¥æ‰¾ç‰¹å®šäºè¯¥æ•°å­—äººçš„å¾…æœºè§†é¢‘
        if avatar_id in ['1', '2', '3']:
            possible_filenames = [
                f"idle_avatar{avatar_id}.mp4",
            ]
        elif avatar_id.startswith('upload_'):
            possible_filenames = [
                "idle_default.mp4",
                "idle_default.webm",
                "idle_breathing.mp4",
                "idle_breathing.webm"
            ]
        else:
            possible_filenames = [
                "idle_default.mp4",
                "idle_default.webm"
            ]

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for filename in possible_filenames:
            video_path = os.path.join(IDLE_VIDEOS_DIR, filename)
            if os.path.exists(video_path):
                logger.info(f"ä¸ºæ•°å­—äºº {avatar_id} æ‰¾åˆ°å¾…æœºè§†é¢‘: {filename}")
                return f"/api/idle_videos/{filename}"  # ä¿®æ”¹ä¸ºæ­£ç¡®çš„ API URL

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šè§†é¢‘ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨è§†é¢‘
        all_videos = get_idle_videos()
        if all_videos:
            return all_videos[0]['url']

        return None

    except Exception as e:
        logger.error(f"è·å–æ•°å­—äººå¾…æœºè§†é¢‘å¤±è´¥: {str(e)}")
        return None


@app.route('/api/idle_videos')
def get_idle_videos_list():
    """
    è·å–å¯ç”¨çš„å¾…æœºè§†é¢‘åˆ—è¡¨
    """
    try:
        avatar_id = request.args.get('avatar_id')

        if avatar_id:
            # è·å–ç‰¹å®šäºè¯¥æ•°å­—äººçš„å¾…æœºè§†é¢‘
            specific_video = get_idle_video_for_avatar(avatar_id)

            if specific_video:
                # è¿”å›ç‰¹å®šè§†é¢‘+å…¶ä»–é€šç”¨è§†é¢‘
                all_videos = get_idle_videos()
                specific_filename = specific_video.split('/')[-1]

                # å°†ç‰¹å®šè§†é¢‘æ”¾åœ¨å‰é¢
                videos = [specific_video]
                for video in all_videos:
                    if video['filename'] != specific_filename:
                        videos.append(video['url'])

                return jsonify({
                    "success": True,
                    "videos": videos,
                    "specific_video": specific_video,
                    "avatar_id": avatar_id,
                    "total": len(videos),
                    "timestamp": datetime.datetime.now().isoformat()
                })

        # é»˜è®¤è¿”å›æ‰€æœ‰è§†é¢‘
        videos = get_idle_videos()
        video_urls = [video['url'] for video in videos]

        return jsonify({
            "success": True,
            "videos": video_urls,
            "total": len(video_urls),
            "timestamp": datetime.datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"è·å–å¾…æœºè§†é¢‘åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "videos": []
        }), 500


@app.route('/api/conversation/reset', methods=['POST'])
def reset_conversation():
    """
    é‡ç½®å¯¹è¯å†å²

    Returns:
        JSON æ ¼å¼çš„æ“ä½œç»“æœ
    """
    global conversation_history
    conversation_history = []
    logger.info("å¯¹è¯å†å²å·²é‡ç½®")
    return jsonify({"success": True, "message": "å¯¹è¯å·²é‡ç½®"})


@app.route('/api/debug', methods=['GET'])
def debug_info():
    """
    è°ƒè¯•ä¿¡æ¯æ¥å£ï¼ˆä»…ç”¨äºå¼€å‘ï¼‰

    Returns:
        JSON æ ¼å¼çš„è°ƒè¯•ä¿¡æ¯
    """
    return jsonify({
        "routes": [str(rule) for rule in app.url_map.iter_rules()],
        "conversation_length": len(conversation_history),
        "deepface_available": DEEPFACE_AVAILABLE,
        "speech_recognition_available": SPEECH_RECOGNITION_AVAILABLE,
        "torch_available": TORCH_AVAILABLE,
        "local_model_loaded": MODEL_LOADED,
        "edge_tts_available": EDGE_TTS_AVAILABLE,
        "sadtalker_image": current_avatar_image,
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/favicon.ico')
def favicon():
    """å¤„ç† favicon è¯·æ±‚ï¼Œé¿å… 404 é”™è¯¯"""
    return '', 204


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† v2.4ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰")
    print("=" * 60)
    print(f"ğŸ“± æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"ğŸ‘¤ å½¢è±¡é€‰æ‹©: http://localhost:5000/select")
    print(f"ğŸ’¬ ä¸»é¡µé¢: http://localhost:5000/index")
    print(f"ğŸ“¤ æ–°å¢åŠŸèƒ½: ç”¨æˆ·å¯ä¸Šä¼ è‡ªå®šä¹‰æ•°å­—äººå½¢è±¡")
    print(f"ğŸ¤ è¯­éŸ³è¾“å…¥: æ”¯æŒï¼ˆéœ€è¦æµè§ˆå™¨æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼‰")
    print(f"ğŸ”Š TTSå¼•æ“: Edge TTSï¼ˆç¦»çº¿å…è´¹ï¼‰")
    print(f"ğŸ”„ å¾…æœºåŠ¨ç”»: æ•°å­—äººç©ºé—²æ—¶ä¼šæœ‰å‘¼å¸å’Œæ‘†åŠ¨åŠ¨ç”»")
    print(f"ğŸ§  æœ¬åœ°æ¨¡å‹: {'å·²åŠ è½½' if MODEL_LOADED else 'åŠ è½½ä¸­' if TORCH_AVAILABLE else 'æœªå®‰è£…'}")
    print(f"â¤ï¸  å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")
    print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€: http://localhost:5000/api/model/status")
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: http://localhost:5000/api/debug")
    print("=" * 60)
    print("å¯ç”¨ API ç«¯ç‚¹:")
    print("  POST /api/analyze        - å¿ƒç†åˆ†æï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰")
    print("  POST /api/analyze_local  - æœ¬åœ°æ¨¡å‹åˆ†æï¼ˆå…¼å®¹æ¥å£ï¼‰")
    print("  POST /api/analyze_emotion - è¡¨æƒ…è¯†åˆ«")
    print("  POST /api/set_avatar     - è®¾ç½®æ•°å­—äººå½¢è±¡")
    print("  POST /api/upload_avatar  - ä¸Šä¼ è‡ªå®šä¹‰å½¢è±¡")
    print("  GET  /api/get_avatars    - è·å–å¯ç”¨å½¢è±¡åˆ—è¡¨")
    print("  POST /api/recognize_speech - è¯­éŸ³è¯†åˆ«")
    print("  GET  /api/health         - å¥åº·æ£€æŸ¥")
    print("  GET  /api/model/status   - æ¨¡å‹çŠ¶æ€")
    print("  GET  /api/conversation/summary - å¯¹è¯æ‘˜è¦")
    print("  POST /api/conversation/reset   - é‡ç½®å¯¹è¯")
    print("=" * 60)

    # æ£€æŸ¥è¯­éŸ³è¯†åˆ«åŠŸèƒ½
    if SPEECH_RECOGNITION_AVAILABLE:
        print("âœ… è¯­éŸ³è¯†åˆ«åŠŸèƒ½å·²å¯ç”¨")
    else:
        print("âš ï¸  è¯­éŸ³è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install SpeechRecognition pydub")

    # æ£€æŸ¥ Edge TTS åŠŸèƒ½
    if EDGE_TTS_AVAILABLE:
        print("âœ… Edge TTS åŠŸèƒ½å·²å¯ç”¨")
    else:
        print("âš ï¸  Edge TTS æœªå¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install edge-tts")

    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹çŠ¶æ€
    if TORCH_AVAILABLE:
        if MODEL_LOADED:
            print("âœ… æœ¬åœ°å¿ƒç†å¤§æ¨¡å‹å·²åŠ è½½")
        else:
            print("ğŸ”„ æœ¬åœ°å¿ƒç†å¤§æ¨¡å‹æ­£åœ¨åå°åŠ è½½ä¸­...")
    else:
        print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡é€‰å›å¤ç³»ç»Ÿ")
        print("    å¦‚éœ€ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œè¯·è¿è¡Œ: pip install torch transformers peft")
    
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)

    # å¯åŠ¨ Flask æœåŠ¡
    app.run(
        debug=True,
        host='0.0.0.0',
        port=5000,
        use_reloader=False  # ç¦ç”¨è‡ªåŠ¨é‡è½½ï¼Œé¿å…æ¨¡å‹é‡å¤åŠ è½½
    )