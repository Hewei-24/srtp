"""
å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - é›†æˆæœåŠ¡å™¨ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰
====================================================

æœ¬æ¨¡å—æä¾›æ”¯æŒæœ¬åœ°å¿ƒç†å¤§æ¨¡å‹çš„ Flask æœåŠ¡ï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. æœ¬åœ° Qwen æ¨¡å‹ + PEFT é€‚é…å™¨åŠ è½½
2. å¿ƒç†å’¨è¯¢å“åº”ç”Ÿæˆ
3. å¤‡é€‰å›å¤ç³»ç»Ÿ

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦ç¦»çº¿è¿è¡Œçš„ç¯å¢ƒ
- å¯¹æ•°æ®éšç§æœ‰è¦æ±‚çš„åœºæ™¯
- éœ€è¦è‡ªå®šä¹‰å¾®è°ƒæ¨¡å‹çš„åœºæ™¯

ä½œè€…: SRTP é¡¹ç›®ç»„
ç‰ˆæœ¬: 1.0
"""

import datetime
import logging
import threading
from typing import Dict, Any, Optional

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== PyTorch ç›¸å…³å¯¼å…¥ ====================
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    TORCH_AVAILABLE = True
    logger.info("PyTorch å’Œ Transformers åº“åŠ è½½æˆåŠŸ")
except ImportError as e:
    TORCH_AVAILABLE = False
    logger.warning(f"PyTorch ç›¸å…³åº“æœªå®‰è£…: {e}")
    logger.warning("æœ¬åœ°æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install torch transformers peft")

# ==================== Flask åº”ç”¨åˆå§‹åŒ– ====================
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# ==================== å…¨å±€æ¨¡å‹å˜é‡ ====================
model = None           # åŠ è½½çš„æ¨¡å‹å®ä¾‹
tokenizer = None       # åˆ†è¯å™¨å®ä¾‹
model_loaded = False   # æ¨¡å‹åŠ è½½çŠ¶æ€æ ‡å¿—

# ==================== é…ç½®å¸¸é‡ ====================
# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆQwen1.5-0.5Bï¼‰
BASE_MODEL_PATH = "C:\\Users\\legion\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B\\snapshots\\8f445e3628f3500ee69f24e1303c9f10f5342a39"

# å¾®è°ƒé€‚é…å™¨è·¯å¾„
ADAPTER_PATH = "outputs/psychology_trained_model"

# æƒ…ç»ªä¸Šä¸‹æ–‡æ˜ å°„
EMOTION_CONTEXT_MAP = {
    'happy':   'ç”¨æˆ·çœ‹èµ·æ¥å¿ƒæƒ…æ„‰å¿«',
    'sad':     'ç”¨æˆ·æƒ…ç»ªä½è½',
    'angry':   'ç”¨æˆ·æœ‰äº›ç”Ÿæ°”',
    'fear':    'ç”¨æˆ·æ„Ÿåˆ°ç´§å¼ ',
    'surprise': 'ç”¨æˆ·æ„Ÿåˆ°æƒŠè®¶',
    'disgust': 'ç”¨æˆ·æœ‰äº›åæ„Ÿ',
    'neutral': 'ç”¨æˆ·æƒ…ç»ªå¹³ç¨³'
}


# ==================== æ¨¡å‹åŠ è½½æ¨¡å— ====================
def load_psychology_model() -> None:
    """
    åŠ è½½å¿ƒç†å¤§æ¨¡å‹

    è¯¥å‡½æ•°åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹å’Œ PEFT é€‚é…å™¨ã€‚
    åŠ è½½å®Œæˆåè®¾ç½® model_loaded æ ‡å¿—ä¸º Trueã€‚
    """
    global model, tokenizer, model_loaded

    if not TORCH_AVAILABLE:
        logger.error("PyTorch æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        return

    try:
        logger.info("æ­£åœ¨åŠ è½½å¿ƒç†å¤§æ¨¡å‹...")
        logger.info(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
        logger.info(f"é€‚é…å™¨è·¯å¾„: {ADAPTER_PATH}")

        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True
        )
        logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜
            device_map="auto",          # è‡ªåŠ¨åˆ†é…è®¾å¤‡
            trust_remote_code=True
        )
        logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

        # åŠ è½½ PEFT é€‚é…å™¨ï¼ˆå¿ƒç†é¢†åŸŸå¾®è°ƒï¼‰
        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        logger.info("PEFT é€‚é…å™¨åŠ è½½å®Œæˆ")

        model_loaded = True
        logger.info("âœ… å¿ƒç†å¤§æ¨¡å‹åŠ è½½å®Œæˆï¼")

    except FileNotFoundError as e:
        logger.error(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        logger.error("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨äºæŒ‡å®šè·¯å¾„")
        model_loaded = False
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model_loaded = False


# ==================== å“åº”ç”Ÿæˆæ¨¡å— ====================
def generate_psychology_response(user_input: str, emotion: str = "neutral") -> str:
    """
    ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¿ƒç†å’¨è¯¢å“åº”

    Args:
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

    Returns:
        ç”Ÿæˆçš„å¿ƒç†å’¨è¯¢å“åº”æ–‡æœ¬
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if not model_loaded or model is None or tokenizer is None:
        return "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™..."

    try:
        # è·å–æƒ…ç»ªä¸Šä¸‹æ–‡æè¿°
        emotion_context = EMOTION_CONTEXT_MAP.get(emotion, EMOTION_CONTEXT_MAP['neutral'])

        # æ„å»ºæç¤ºè¯
        prompt = f"""ã€å¿ƒç†åŠ©æ‰‹ã€‘æŒ‡ä»¤ï¼šè¯·ä»¥ä¸“ä¸šå¿ƒç†åŠ©æ‰‹çš„èº«ä»½å›åº”ç”¨æˆ·çš„å¿ƒç†é—®é¢˜
è¾“å…¥ï¼šç”¨æˆ·è¯´ï¼š{user_input}
æƒ…ç»ªçŠ¶æ€ï¼š{emotion_context}
å›ç­”ï¼š"""

        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,      # æœ€å¤§ç”Ÿæˆ token æ•°
                do_sample=True,          # å¯ç”¨é‡‡æ ·
                temperature=0.7,         # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
                top_p=0.9,               # æ ¸é‡‡æ ·å‚æ•°
                repetition_penalty=1.1,  # é‡å¤æƒ©ç½š
                pad_token_id=tokenizer.eos_token_id
            )

        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰æç¤ºè¯ï¼‰
        generated_response = response[len(prompt):].strip()

        # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„åç»­å¯¹è¯ï¼‰
        if "ç”¨æˆ·ï¼š" in generated_response:
            generated_response = generated_response.split("ç”¨æˆ·ï¼š")[0].strip()
        if "ã€å¿ƒç†åŠ©æ‰‹ã€‘" in generated_response:
            generated_response = generated_response.split("ã€å¿ƒç†åŠ©æ‰‹ã€‘")[0].strip()

        return generated_response if generated_response else "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œè¯·ç»§ç»­å‘Šè¯‰æˆ‘æ›´å¤šã€‚"

    except Exception as e:
        logger.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å†è¯•ä¸€æ¬¡ã€‚"


def generate_fallback_response(user_input: str, emotion: str = "neutral") -> str:
    """
    ç”Ÿæˆå¤‡é€‰å›å¤ï¼ˆå½“æ¨¡å‹ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰

    Args:
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªç±»å‹

    Returns:
        åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡é€‰å›å¤
    """
    # æƒ…ç»ªå‰ç¼€æ˜ å°„
    emotion_prefix_map = {
        'happy':   'è™½ç„¶æ‚¨æƒ…ç»ªè¿˜å¥½ï¼Œä½†',
        'sad':     'åœ¨æƒ…ç»ªä½è½æ—¶ï¼Œ',
        'angry':   'åœ¨çƒ¦èºçš„æ—¶å€™ï¼Œ',
        'fear':    'æ„Ÿåˆ°ä¸å®‰æ—¶ï¼Œ',
        'surprise': 'æ„Ÿåˆ°æ„å¤–æ—¶ï¼Œ',
        'disgust': 'æ„Ÿåˆ°ä¸é€‚æ—¶ï¼Œ',
        'neutral': ''
    }

    # å…³é”®è¯å“åº”æ˜ å°„
    keyword_responses = {
        'å‹åŠ›': "é¢å¯¹å‹åŠ›æ—¶ï¼Œå»ºè®®ï¼š1.åˆ¶å®šåˆç†è®¡åˆ’ 2.å­¦ä¼šè¯´'ä¸' 3.é€‚å½“æ”¾æ¾ 4.ä¿æŒè¿åŠ¨",
        'ç„¦è™‘': "åº”å¯¹ç„¦è™‘ï¼š1.æ·±å‘¼å¸ç»ƒä¹  2.æ­£å¿µå†¥æƒ³ 3.ä¸æœ‹å‹å€¾è¯‰ 4.å¯»æ±‚ä¸“ä¸šå¸®åŠ©",
        'å¤±çœ ': "æ”¹å–„ç¡çœ ï¼š1.è§„å¾‹ä½œæ¯ 2.ç¡å‰æ”¾æ¾ 3.é¿å…å’–å•¡å›  4.èˆ’é€‚ç¯å¢ƒ",
        'æƒ…ç»ª': "æƒ…ç»ªç®¡ç†ï¼š1.è¯†åˆ«æƒ…ç»ª 2.æ¥çº³æ„Ÿå— 3.å¥åº·è¡¨è¾¾ 4.è½¬ç§»æ³¨æ„åŠ›",
        'æŠ‘éƒ': "æƒ…ç»ªä½è½æ—¶ï¼š1.å¯»æ±‚æ”¯æŒ 2.ä¿æŒæ´»åŠ¨ 3.ä¸“ä¸šå’¨è¯¢ 4.è€å¿ƒå¯¹å¾…è‡ªå·±",
        'å­¦ä¹ ': "å­¦ä¹ å›°æ‰°ï¼š1.åˆ¶å®šè®¡åˆ’ 2.åŠ³é€¸ç»“åˆ 3.å¯»æ±‚å¸®åŠ© 4.ä¿æŒä¿¡å¿ƒ",
        'äººé™…': "äººé™…å…³ç³»ï¼š1.çœŸè¯šæ²Ÿé€š 2.æ¢ä½æ€è€ƒ 3.ä¿æŒè¾¹ç•Œ 4.å¯»æ±‚å…±è¯†"
    }

    # è·å–æƒ…ç»ªå‰ç¼€
    emotion_prefix = emotion_prefix_map.get(emotion, '')

    # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
    for keyword, response in keyword_responses.items():
        if keyword in user_input:
            return f"{emotion_prefix}{response}"

    # é€šç”¨å›å¤
    return "æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ã€‚ä½œä¸ºå¿ƒç†åŠ©æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥æ›´è¯¦ç»†åœ°æè¿°å…·ä½“æƒ…å†µï¼Œè¿™æ ·æˆ‘èƒ½æä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„å¸®åŠ©ã€‚"


# ==================== API è·¯ç”± ====================

@app.route('/')
def index():
    """
    é¦–é¡µè·¯ç”± - æä¾›å‰ç«¯é¡µé¢

    Returns:
        æ¸²æŸ“çš„ HTML æ¨¡æ¿
    """
    return render_template('index.html')


@app.route('/api/analyze_psychology', methods=['POST'])
def analyze_psychology():
    """
    å¿ƒç†åˆ†æ API æ¥å£

    è¯·æ±‚ä½“:
        - message: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        - detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆå¯é€‰ï¼Œé»˜è®¤ neutralï¼‰

    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        data = request.get_json()
        user_input = data.get('message', '').strip()
        detected_emotion = data.get('detected_emotion', 'neutral')

        logger.info(f"æ”¶åˆ°å¿ƒç†åˆ†æè¯·æ±‚ - æƒ…ç»ª: {detected_emotion}, è¾“å…¥: {user_input[:50]}...")

        # éªŒè¯è¾“å…¥
        if not user_input:
            return jsonify({
                "success": False,
                "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º"
            }), 400

        # ç”Ÿæˆå“åº”
        if model_loaded:
            response = generate_psychology_response(user_input, detected_emotion)
            model_source = "local_psychology_model"
        else:
            response = generate_fallback_response(user_input, detected_emotion)
            model_source = "fallback_system"

        # å¦‚æœå“åº”å¤ªçŸ­ï¼Œä½¿ç”¨å¤‡é€‰å›å¤
        if len(response) < 20:
            response = generate_fallback_response(user_input, detected_emotion)
            model_source = "fallback_system"

        return jsonify({
            "success": True,
            "response": response,
            "detected_emotion": detected_emotion,
            "model_source": model_source,
            "timestamp": datetime.datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"å¿ƒç†åˆ†ææ¥å£é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "å¿ƒç†åˆ†æå¤±è´¥"
        }), 500


@app.route('/api/model_status', methods=['GET'])
def get_model_status():
    """
    æ¨¡å‹çŠ¶æ€æŸ¥è¯¢æ¥å£

    Returns:
        JSON æ ¼å¼çš„æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    """
    return jsonify({
        "model_loaded": model_loaded,
        "torch_available": TORCH_AVAILABLE,
        "status": "ready" if model_loaded else ("loading" if TORCH_AVAILABLE else "unavailable"),
        "timestamp": datetime.datetime.now().isoformat()
    })


@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£

    Returns:
        JSON æ ¼å¼çš„æœåŠ¡å¥åº·çŠ¶æ€
    """
    return jsonify({
        "status": "healthy",
        "service": "å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - æœ¬åœ°æ¨¡å‹ç‰ˆ",
        "version": "1.0",
        "model_loaded": model_loaded,
        "timestamp": datetime.datetime.now().isoformat()
    })


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("å¤§å­¦ç”Ÿå¿ƒç†åˆ†ææ•°å­—äººä»£ç† - æœ¬åœ°æ¨¡å‹ç‰ˆ v1.0")
    print("=" * 60)
    print(f"ğŸ“± æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"â¤ï¸  å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")
    print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€: http://localhost:5000/api/model_status")
    print("=" * 60)

    if TORCH_AVAILABLE:
        # åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡æœåŠ¡å¯åŠ¨
        print("ğŸ”„ æ­£åœ¨åå°åŠ è½½å¿ƒç†å¤§æ¨¡å‹...")
        model_thread = threading.Thread(target=load_psychology_model, daemon=True)
        model_thread.start()
    else:
        print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡é€‰å›å¤ç³»ç»Ÿ")

    print("=" * 60)
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)

    # å¯åŠ¨ Flask æœåŠ¡
    app.run(
        debug=True,
        host='0.0.0.0',
        port=5000,
        use_reloader=False  # ç¦ç”¨è‡ªåŠ¨é‡è½½ï¼Œé¿å…æ¨¡å‹é‡å¤åŠ è½½
    )
